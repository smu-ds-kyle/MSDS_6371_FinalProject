---
title: "STATS 6371 Final Project Report"
author: "Kyle Evans and Eric Graham"
date: "`r Sys.Date()`"
output: 
  word_document
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(skimr)
library(ggplot2)
```

# Introduction

Given a dataset of home prices and features in Ames, Iowa, we set out to analyze existing factors impacting home prices and look for predictive models that can accurately predict home prices. Our first analysis (Analysis 1) is descriptive in nature: specifically, we are interested in understanding the effect that living area has on home prices in three neighborhoods in Ames. Our second analysis (Analysis 2) is predictive: we explored several linear regression models of varying size and complexity in an effort to find the best model for predicting home prices.

## Note on Code Appendix and Figures

The assignment calls for an appendix to include our code, but because the code for our project is quite long, we have instead opted to include the entire notebook on our project github, which can be found [here](https://github.com/smu-ds-kyle/MSDS_6371_FinalProject/blob/main/Project.Rmd).

In order to keep the written paper at seven pages, we have moved our figures to the appendix to this document.

# Data Description

Our dataset includes 1,460 rows and 81 features, 43 of which are characters (categorical variables) and 38 of which are numeric variables. In addition to including the sales price for each observation, the 80 other features cover a variety of aspects of the home, including the neighborhood, living area (in square footage), the number of bedrooms, the size of the garage, the lot frontage, and many others. This is a dataset that is very popular for predictive modeling and has been used in many Kaggle competitions.

To that end, we were also provided with a "test" dataset of 1459 observations which omits the sales price; we used this test data to validate our predictive models from Analysis 2 on Kaggle, and will include our Kaggle scores in evaluating those models.

# Analysis 1: Descriptive Analysis

## Introduction

Our first analysis examines the relationship between LogSalePrice and GrLivArea (measured in increments of 100 square feet) for homes in the NAmes, Edwards, and BrkSide neighborhoods. Specifically, the goal is to determine whether the relationship varies by neighborhood and to provide estimates with confidence intervals.

### Transformation of SalePrice

An initial look at the data shows that the relationship between SalePrice and GrLivArea is positive and linear for all neighborhoods, with some outliers. However, because the distribution of SalePrice was right-skewed, we decided to use a log transformation of SalePrice in our analysis. See Figure 1 and 2in the appendix.

### Outliers in Edwards Neighborhood

We identified two outliers (observations 524 and 1299) in the Edwards neighborhood for which an exceptionally large living area was associated with a lower-than-expected sale price. Extensive analysis of these influential points can be found in our code appendix. See figure 3 in the appendix.

We removed both of these outliers from the dataset. For the sake of presentation to the client, we would disclose these outliers, and note that the model is based on a dataset that doesn't include them. By removing these two data points, we increased the adjusted R2 of our model by 8%. In keeping with the idea that "all models are wrong but some are useful", we believe that the best course of action is to present this model to the company with the caveat that it should only be used with houses 3000 square feet or less. See figure 4 in the appendix.

### General Model

We fit the general model as follows:

$$\log(\text{SalePrice}) = \beta_0 + \beta_1 \times \text{GrLivArea}_{\text{BrkSide}} + \beta_2 \times \text{GrLivArea}_{\text{Edwards}} + \beta_3 \times \text{GrLivArea}_{\text{NAmes}} + \epsilon$$

## Results

The results of our model are shown in Figure 5 in the appendix, as well as the code in our Github.

The results show that the relationship between GrLivArea and LogSalePrice is positive for all neighborhoods, and the p-values of all coefficients is statistically signifcant. However, the magnitude of the effect varies by neighborhood: the effect of GrLivArea on LogSalePrice is smallest in the NAmes neighborhood and largest in the Edwards neighborhood. Our neighboorhood-specific models are as follows:

### BrkSide

$$\log(\text{SalePrice}) = \beta_0 + \beta_1 \times \text{GrLivArea}$$

$$\log(\text{SalePrice}) = 10.79 + 0.000738 \times \text{GrLivArea}$$

### Edwards

$$\log(\text{SalePrice}) = (\beta_0 + \beta_{\text{Edwards}}) + \beta_2 \times \text{GrLivArea}$$

$$\log(\text{SalePrice}) = (10.79 + 0.2339) + 0.0005387 \times \text{GrLivArea}$$

### NAmes

$$\log(\text{SalePrice}) = (\beta_0 + \beta_{\text{NAmes}}) + \beta_3 \times \text{GrLivArea}$$

$$\log(\text{SalePrice}) = (10.79 + 0.6517) + 0.0003241 \times \text{GrLivArea}$$

## Assumptions

### Linearity

The plot of residuals against fitted values hows no apparent pattern, which suggests that the assumption of linearity is met (see Figure 6 in the appendix).

### Normality

The Q-Q plot indicates that residuals closely follow a normal distribution, with some deviation at the extremes (see Figure 7 in the appendix).

### Variance

The spread of residuals in the residuals versus fitted plot (Figure 6) is relatively consistent across fitted values, which suggests that the assumption of constant variance is met.

### Independence

We will assume that observations are independent enough to meet this assumption, however there is a possible cluster effect due to houses being in the same neighborhood. By adding the Neighborhood variable to the model, perhaps the violation, if present, is somewhat mitigated.

### Influential Points Analysis

After removing the significant outliers mentioned above (observations 524 and 1299), the Cook's D and outlier-leverage diagnostics reveal a few remaining influential points. However, these points were less impactful than those already removed, and while they are outside the normal range, we didn't deem them extreme enough to warrant removal (see Figure 7 and 8 in the appendix).

## Parameter Estimates and Model Interpretation

The model estimates suggest significant relationships between log-transformed sales price and both neighborhood and living area. The intercept 10.79 represents the average log-sale price for homes in the BrkSide neighborhood (our reference category) when the living area is zero. This is obviously not practically feasible, and it exists solely as a parameter in the model.

The coefficients for the Edwards (0.2339) and NAmes (0.6517) neighborhoods indicate higher baseline log-sale prices compared to BrkSide. Additionally, the interaction terms for living area show positive effects on log-sale price, with the strongest effect in BrkSide (0.000738) and decreasing effects in Edwards (0.000539) and NAmes (0.000324).

Our 95% confidence intervals for the coefficients are as follows (meaning that we are 95% certain that the true coefficient is within the given range):

| Term                          | Confidence Interval (95%) |
|-------------------------------|---------------------------|
| Intercept                     | 10.63 and 10.95.          |
| NeighborhoodEdwards           | 0.0210 and 0.4468.        |
| NeighborhoodNAmes             | 0.4708 and 0.8327.        |
| NeighborhoodBrkSide:GrLivArea | 0.000611 and 0.000866.    |
| NeighborhoodEdwards:GrLivArea | 0.000432 and 0.000645.    |
| NeighborhoodNAmes:GrLivArea   | 0.000264 and 0.000384.    |

## Conclusion

It is estimated that for every 100 square feet increase in living area, the median sales price of a home in Brookside increases by 7.38% (p-value \< 0.0001). A 95% confidence interval for the true median increase in sales price is between 6.11% and 8.66%.

It is estimated that for every 1100 square feet increase in living area, the median sales price of a home in Northwest Ames increases by 3.24% (p-value \< 0.0001). A 95% confidence interval for the true median increase in sales price is between 2.64% and 3.84%.

It is estimated that for every 100 square feet increase in living area, the median sales price of a home in Edwards increases by 5.39% percent (p-value \< 0.0001). A 95% confidence interval for the true median increase in sales price is between 4.33% and 6.45%.

# Analysis 2: Predictive Analysis

We now turn to the predictive analysis of home prices in Ames, Iowa. We will explore several linear regression models of varying size and complexity in an effort to find the best model for predicting home prices. The below table is contains the metrics for the models we tested and what follows after is a brief discussion of each model:

## Linear Model Comparison Table

Note, that metrics were pulled from the resulting CARET models after LOOVC, not directly from the LM() function. For consistency, unless otherwise noted, all models were ran without the same outliers found above, 1299 and 524. The names in the tables should match up nicely in the outline of the project.rmd.

| Model                        | Adjusted RÂ² | AIC      | PRESS    | Kaggle Score |
|-------------------|--------------|--------------|--------------|--------------|
| SLR-M1: GarageArea           | 0.4301      | 647.7850 | 133.0714 | 0.31527      |
| MLR:M0: GrLivArea + FullBath | 0.5551      | 287.6378 | 103.9207 | 0.29097      |
| MLR:M1: 50 predictors        | 0.9370      | -2406.90 | 19.74    | 0.14395      |
| MLR:M3: 25 predictors        | 0.9011      | -1845.19 | 24.50    | 0.14914      |
| MLR:M6  18 predictors        | 0.9057      | -1973.69 | 21.57    | 0.13482      |

## Model 1: Simple Linear Regression

Our simplest model is a linear regression of SalePrice on GarageArea. This model seeks to predict home prices based on the size of the garage.

### General Model

We fit the general model as follows:

$$\log(\text{SalePrice}) = \beta_0 + \beta_1 \times \text{GarageArea} + \epsilon$$

### Results

As shown in the code in our appendix, we implemented Leave One Out Cross-Validation on our predictive models to evaluate their performance. The results of our model are also shown in Figure 9 in the appendix.

The results show that the relationship between GarageArea and LogSalePrice is positive, and the p-values for both the coefficient and intercept are statistically significant. The model has an adjusted R2 of 0.4301, which means that 43% of the variance in log-transformed sale price can be explained by the size of the garage. This is less promising than our descriptive model, but still a good start.

The linear regression of our equation is as follows:

$$\log(\text{SalePrice}) = 11.44 + 0.001236 \times \text{GarageArea}$$

### Assumptions and Influential Points Analysis

#### Linearity

The plot of residuals against fitted values shows no apparent pattern, indicating that the assumption of linearity is reasonably satisfied (see Figure 10 in the appendix).

#### Normality

The Q-Q plot suggests that residuals follow a normal distribution, however some deviation is observed at the tails (See Figure 11 in the appendix).

#### Variance

The spread of residuals in the residuals plot (shown in Figure 10) is relatively uniform across fitted values, indicating that the assumption of constant variance (homoscedasticity) is met.

#### Independence

We will assume that observations are independent enough to meet this assumption, however there is a possible cluster effect due to houses being in the same neighborhood. By adding the Neighborhood variable to the model, perhaps the violation, if present, is somewhat mitigated.

#### Influential Points Analysis

The Cook's D and outlier-leverage plots identify a few influential points (such as observations 1061 and 1190), but they they appear to be minor violations of the assumptions and were not removed (see Figures 12 and 13 in the appendix).

## Model 2: Multiple Linear Regression (GrLivArea + FullBath)

Our second model is a multiple linear regression of SalePrice on GrLivArea and FullBath. This model seeks to predict home prices based on the living area and number of full bathrooms.

### General Model

$$\log(\text{SalePrice}) = \beta_0 + \beta_1 \times \text{GrLivArea} + \beta_2 \times \text{FullBath} + \epsilon$$

### Results

As with the simple linear model, we implemented Leave One Out Cross-Validation on our first multiple linear model to evaluate its performance. The results of our model are shown in Figure 14 in the appendix (as well as the full code on Github).

This results indicate that, on average, for every additional square foot of living area, the log of the sale price increases by 0.000458, and for each additional full bathroom, the log of the sale price increases by 0.1631, holding other variables constant. The intercept (11.08) represents the baseline log-sale price when both predictors are zero. This is obviously not practically feasible (as a house must have at least some living area, and it is unusual for a single family home to have no full bathrooms, though I have made do with some very small bathrooms!), thus it exists solely as a parameter in the model.

The adjusted R2 of this model is 0.5501, which means that 55% of the variance in log-transformed sale price can be explained by the size of the living area and the number of full bathrooms. This is a significant improvement over the simple linear model.

The linear regression of our equation is as follows:

$$\log(\text{SalePrice}) = 11.08 + 0.000458 \times \text{GrLivArea} + 0.1631 \times \text{FullBath}$$

### Assumptions and Influential Points Analysis

#### Linearity

The plot of residuals against fitted values shows no apparent pattern, indicating that the assumption of linearity is reasonably satisfied (see Figure 15 in the appendix).

#### Normality

The Q-Q plot suggests that residuals follow a normal distribution, however some moderate deviation is observed at the tails (See Figure 16 in the appendix).

#### Variance

The spread of residuals in the residuals plot (shown in Figure 17 in the appendix) is relatively uniform across fitted values, indicating that the assumption of constant variance (homoscedasticity) is met.

#### Independence

No patterns or unusual are visible in the residuals plot (shown in Figure 17 in the appendix), which supports the assumption of independence.

#### Influential Points Analysis

The Cook's D plot and the outlier-leverage plot identify a few points with high leverage or influence, such as observation 54. While these points do exceed the threshold for influence, we didn't think that they severely distorted the model, and we did not remove them (see Figures 17 and 18 in the appendix).

## Model 3:

This model was found using the MASS package for step-wise feature selection targeting AIC. We will break from the above pattern and not include the formula for brevity's sake. In total there were 50 predictors in this model.

### Assumptions and Influential Points Analysis

Figures 19-21 in the appendix

#### Linearity

The plot of the residuals has a decently normal cloud around 0, but there is slight evidence of a bigger variance at the beginning and end of the plots.

#### Normality

The Q-Q plot suggests that residuals follow a normal distribution moderately well, except for the tails of the distribution. This will be common theme going forward.

#### Variance

The spread of the residuals as mention above provides some evidence of heteroscedasticity.

#### Independence

We will assume that observations are independent enough to meet this assumption, however there is a possible cluster effect due to houses being in the same neighborhood. By adding the Neighborhood variable to the model, perhaps the violation, if present, is somewhat mitigated.

#### Influential Points Analysis

There are numerous influential points in this model, however, this model predicts better than simpler models with less. "All models are wrong, but some are useful."

## Model 4:

This model was found by first looking at scatter plots and box plots of every variable with the SalePrice.  After taking note of "promising" variables, we ran a step-wise feature selection with the OLSRR package. This model has 25 predictors. As the 7 page limit is approaching and this is a bonus model, I will forgo a detail analysis of the assumptions. Suffice it to say that this model, while achieving our second highest Kaggle score, violated many of the assumptions of linear regression. Namely, there was decreasing variance, left skew of the residuals, and many influential points.  Yet again, "All models are wrong, but some are useful." For plots and details, please see project.rmd.

## Model 5:

In model 5, we attempted to use feature engineering, polynomial terms, and ordered factors to improve our linear model. We also used scatter plots to visualize outliers across all variables and removed them. Ultimately this proved to be the best strategy as it resulted in our best Kaggle score.  Much like Model 4 above, this model also violated many of the assumptions of linear regression, i.e. decreasing variance, left skew of the residuals, and many influential points. For full details on feature engineering and the model, please see project.rmd.

## Conclusion:

In this project, we have explored the relationship between home prices and various features in the Housing Prices data set.  We provided the client with a model to predict the median sale price increase as the general living square footage of a house increases, controlled for the three different neighborhood they service.  In addition, we gained experience exploring the predictive power of various different models, from simple linear regression to multiple linear regression, comparing various metrics and diagnostic plots. Perhaps more so than anything, we finally learned the practical, hands-on application of "All models are wrong, but some are useful."  We found that even when assumptions are violated, and influential points are present, a model can still be useful if it predicts well.   At the time of this writing, our best Kaggle Score (.13471) places our model in the top 30% on the leaderboard.  Here's hoping we get another chance at this in Stats 2!

# Appendix: Figures

![Figure 1: Scatterplot of SalePrice and GrLivAr](images/00_a1_saleprice.png)

![Figure 2: Scatterplot of SalePriceLog and GrLivAr](images/00_a1_logsaleprice.png)

![Figure 3: Scatterplot of SalePriceLog and GrLivAr by Neighborhood](images/00_a1_outliers.png)

![Figure 4: Scatterplot of SalePriceLog and GrLivAr by Neighborhood](images/00_a1_outliers2.png)

![Figure 5: Summary of Descrtiptive Model](images/00_a1_summary.png)

![Figure 6: Residuals vs Fitted, Descriptive Model](images/03_a1_residual_plot.png)

![Figure 7: Cook's D Plot for Descriptive Model](images/08_a1_cooks.png)

![Figure 8: Outlier-Leverage Plot for Descriptive Model](images/10_a1_outliers.png)

![Figure 9: Summary of Simple Linear Model](images/00_a2_slr__summary.png)

![Figure 10: Residuals vs Fitted for Simple Linear Model](images/12_a2_slr_residuals.png)

![Figure 11: Q-Q Plot for Simple Linear Model](images/13_a2_slr_qq.png)

![Figure 12: Cook's D Plot for Simple Linear Model](images/17_a2_slr_cooks_bar.png)

![Figure 13: Outliers-Leverage Plot for Simple Linear Model](images/19_a2_slr_outliers.png)

![Figure 14: Summary of GrLivArea + FullBath Model](images/00_a2_mlr0__summary.png)

![Figure 15: Residuals vs Fitted for GrLivArea + FullBath Model](images/21_a2_mlr_0_residuals.png)

![Figure 16: Q-Q Plot for GrLivArea + FullBath Model](images/22_a2_mlr_0_qq.png)

![Figure 17: Cook's D Plot for GrLivArea + FullBath Model](images/26_a2_mlr_0_cooks_bar.png)

![Figure 18: Outliers-Leverage Plot for GrLivArea + FullBath Model](images/28_a2_mlr_0_outliers.png)

![Figure 19: Model 3 Scatter Plot of Residuals](images/29_a2_mlr_1_residuals.png)

![Figure 20: Model 3 QQ Plot of Residuals](images/30_a2_mlr_1_qq.png)

![Figure 21: Model 3 Cook's D](images/31_a2_mlr_1_cooksd.png)
