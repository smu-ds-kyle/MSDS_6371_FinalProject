---
title: "Project"
author: "Kyle Evans and Eric Graham"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Helper Functions

```{r}

library(car)
library(stringr)
library(doParallel)
library(caret)
library(olsrr)
library(bestNormalize)
library(naniar)

generate_diagnosticPlots <- function(model){
  
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model)
par(mfrow = c(1, 1))  # 

plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

qqnorm(model$residuals,
       main = "Normal Q-Q Plot")
qqline(model$residuals, col = "red")

influencePlot(model)

hist(model$residuals,
     breaks = 20,
     xlab = "Residuals",
     main = "Histogram of Residuals",
     col = "lightblue")

ols_plot_cooksd_chart(model)
ols_plot_cooksd_bar(model)
ols_plot_dffits(model)
ols_plot_resid_lev(model)
}

impute_missing_by_group <- function(data, group_col) {
  # Ensure group_col exists
  if (!group_col %in% names(data)) {
    stop("The grouping column does not exist in the dataset.")
  }
  
  # Function to calculate mode
  mode_function <- function(x) {
    x_non_na <- x[!is.na(x)]  # Remove NA values
    if (length(x_non_na) == 0) return(NA)  # Return NA if all values are missing
    return(as.character(names(sort(table(x_non_na), decreasing = TRUE))[1]))
  }
  
  # Impute missing values
  data <- data %>%
    group_by(!!sym(group_col)) %>% 
    mutate(
      # Impute numeric columns with group median
      across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)),
      # Impute categorical columns with group mode
      across(where(is.factor), ~ ifelse(is.na(.), mode_function(.), as.factor(.))),
      across(where(is.character), ~ ifelse(is.na(.), mode_function(.), as.character(.)))
      
    ) %>%
    ungroup()
  
  return(data)
}

generate_model_stats <- function(model, data_set){

terms <- all.vars(terms(model))

#needed for some inconsistencies I introduced in my project.  used SalePrice and SalePriceLog as response variables at different times, messes with me now unfortunately
terms <- setdiff(terms, c('SalePrice','SalePriceLog'))

data_M <- data_set %>% dplyr::select(all_of(terms), 'SalePriceLog')

num_cores <- parallel::detectCores() - 2
cl <- makeCluster(num_cores)
registerDoParallel(cl)

ctrl <- trainControl(method = "LOOCV", allowParallel = TRUE, savePredictions = "all")
model <- train(SalePriceLog ~ ., data = data_M, method = "lm", trControl = ctrl)

cv_predictions <- model$pred
residuals <- cv_predictions$obs - cv_predictions$pred


print(summary(model))
print(model)

aic <- AIC(model$finalModel)
print(str_glue("AIC: {aic} \n"))

bic_value <- BIC(model$finalModel)
print(str_glue("BIC: {bic_value} \n"))

pressStat <- sum(residuals^2)
print(str_glue("PRESS: {pressStat} \n"))

stopCluster(cl)

return(model)
}

preprocess_data <- function(data_t, orderFactors = FALSE){
  
  #data_t = testData
  
  data_clean <- data_t %>% 
  rename(
    FirstFlrSF = '1stFlrSF',
    SecondFlrSF = '2ndFlrSF',
    ThirdSsnPorch = '3SsnPorch'
  )

#these look like factors
data_clean <- data_clean %>% 
  mutate(
    OverallQual = as.factor(OverallQual),
    OverallCond = as.factor(OverallCond),
    MSSubClass = as.factor(MSSubClass),
    PoolQC = ifelse(is.na(PoolQC), 'None', PoolQC),
    MiscFeature = ifelse(is.na(MiscFeature), 'None', MiscFeature),
    Alley = ifelse(is.na(Alley), 'None', Alley),
    Fence = ifelse(is.na(Fence), 'None', Fence),
    FireplaceQu = ifelse(is.na(FireplaceQu), 'None', FireplaceQu),
    GarageArea = ifelse(is.na(GarageArea), 0, GarageArea),
    GarageYrBlt = ifelse(is.na(GarageYrBlt) & is.na(GarageType), 0, GarageYrBlt),
    GarageType = ifelse(is.na(GarageType), 'None', GarageType),
    GarageFinish = ifelse(is.na(GarageFinish), 'None', GarageFinish),
    GarageQual = ifelse(is.na(GarageQual), 'None', GarageQual),
    GarageCond = ifelse(is.na(GarageCond), 'None', GarageCond),
    BsmtQual = ifelse(is.na(BsmtQual), 'None', BsmtQual),
    BsmtCond = ifelse(is.na(BsmtCond), 'None', BsmtCond),
    BsmtExposure = ifelse(is.na(BsmtExposure), 'None', BsmtExposure),
    BsmtFinType2 = ifelse(is.na(BsmtFinType2), 'None', BsmtFinType2),
    BsmtFinType1 = ifelse(is.na(BsmtFinType1), 'None', BsmtFinType1),
    MasVnrArea = ifelse(MasVnrType == 'None' & is.na(MasVnrArea), 0, MasVnrArea),
    NeighborhoodClass = case_when(
    Neighborhood %in% c("NoRidge", "NridgHt", "StoneBr") ~ "Wealthy",
    Neighborhood %in% c("Timber", "Veenker", "Somerst", "ClearCr", 
                        "Crawfor", "CollgCr", "Blmngtn", "Gilbert", 
                        "NWAmes", "SawyerW") ~ "Middle",
    TRUE ~ "Lower"
    )
  )

#impute lot frontage with the median lotfrantage by neighborhood
data_clean <- data_clean %>%
  group_by(Neighborhood) %>%
  mutate(LotFrontage = ifelse(is.na(LotFrontage), median(LotFrontage, na.rm = TRUE), LotFrontage)) %>%
  ungroup() 

#assuming masvnrarea na means no masonry veneer
data_clean <- data_clean %>% 
  mutate(
    MasVnrArea = ifelse(is.na(MasVnrArea), 0, MasVnrArea),
    MasVnrType = ifelse(is.na(MasVnrType), 'None', MasVnrType)
  )

#impute the rest of the missing values, to the median for numeric/ mode for catagorical, based on neighborhood
data_clean <- impute_missing_by_group(data_clean,'Neighborhood')


data_clean <- data_clean %>% 
  mutate(
    HasGarage = ifelse(GarageType == 'None', 'No', 'Yes'),
    HasBsmt = ifelse(BsmtQual == 'None', 'No', 'Yes'),
    HasFence = ifelse(Fence == 'None', 'No', 'Yes'),
    HasPool = ifelse(PoolQC == 'None', 'No', 'Yes'),
    HasMiscFeature = ifelse(MiscFeature == 'None', 'No', 'Yes'),
    HasAlley = ifelse(Alley == 'None', 'No', 'Yes'),
    HasFireplace = ifelse(FireplaceQu == 'None', 'No', 'Yes'),
    HasSecondFloor = ifelse(SecondFlrSF == 0, 'No', 'Yes'),
    HasVnr = ifelse(MasVnrType == 'None' & MasVnrArea == 0, 'No', 'Yes')
    
  )

#ordinal factors
data_clean <- data_clean %>% 
  mutate(
    ExterQual = factor(ExterQual, levels = c('Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    ExterCond = factor(ExterCond, levels = c('Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    BsmtQual = factor(BsmtQual, levels = c('None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    BsmtCond = factor(BsmtCond, levels = c('None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    BsmtExposure = factor(BsmtExposure, levels = c('None', 'No', 'Mn', 'Av', 'Gd'), ordered = TRUE),
    BsmtFinType1 = factor(BsmtFinType1, levels = c('None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'), ordered = orderFactors),
    BsmtFinType2 = factor(BsmtFinType2, levels = c('None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'), ordered = orderFactors),
    HeatingQC = factor(HeatingQC, levels = c('Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    KitchenQual = factor(KitchenQual, levels = c('Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    Functional = factor(Functional, levels = c('Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'), ordered = orderFactors),
    FireplaceQu = factor(FireplaceQu, levels = c('None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    GarageFinish = factor(GarageFinish, levels = c('None', 'Unf', 'RFn', 'Fin'), ordered = orderFactors),
    GarageQual = factor(GarageQual, levels = c('None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    GarageCond = factor(GarageCond, levels = c('None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    PoolQC = factor(PoolQC, levels = c('None', 'Fa', 'TA', 'Gd', 'Ex'), ordered = orderFactors),
    Fence = factor(Fence, levels = c('None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'), ordered = orderFactors),
    NeighborhoodClass = factor(NeighborhoodClass, levels = c('Lower', 'Middle', 'Wealthy'), ordered = orderFactors),
    PavedDrive = factor(PavedDrive, levels = c('N', 'P', 'Y'), ordered = orderFactors)
  )

#ordinal factors to numbers, cause why not

data_clean <- data_clean %>% 
  mutate(
    ExterQual_Num = as.numeric(ExterQual),
    ExterCond_Num = as.numeric(ExterCond),
    BsmtQual_Num = as.numeric(BsmtQual),
    BsmtCond_Num = as.numeric(BsmtCond),
    BsmtExposure_Num = as.numeric(BsmtExposure),
    BsmtFinType1_Num = as.numeric(BsmtFinType1),
    BsmtFinType2_Num = as.numeric(BsmtFinType2),
    HeatingQC_Num = as.numeric(HeatingQC),
    KitchenQual_Num = as.numeric(KitchenQual),
    Functional_Num = as.numeric(Functional),
    FireplaceQu_Num = as.numeric(FireplaceQu),
    GarageFinish_Num = as.numeric(GarageFinish),
    GarageQual_Num = as.numeric(GarageQual),
    GarageCond_Num = as.numeric(GarageCond),
    PoolQC_Num = as.numeric(PoolQC),
    Fence_Num = as.numeric(Fence),
    NeighborhoodClass_Num = as.numeric(NeighborhoodClass),
    PavedDrive_Num = as.numeric(PavedDrive)
  )



#chars to factors
data_clean <- data_clean %>%
  mutate_if(is.character, as.factor)


#more feature creation after everything has been imputed. 

data_clean <- data_clean %>% 
  mutate(
    TotalAreaSF = TotalBsmtSF + FirstFlrSF + SecondFlrSF + GarageArea,
    TotalBaths = FullBath + (.5 * HalfBath) + BsmtFullBath + (.5 * BsmtHalfBath),
    YearsSinceRemodel = YearRemodAdd - YearBuilt,
    QualityTotalSF = TotalAreaSF * OverallQual,
    QualityTotalSF_SQ = (TotalAreaSF^2) * OverallQual,
    QualityBsmtSF = BsmtQual_Num * BsmtFinSF1,
    QualityBsmtSF_SQ = (BsmtFinSF1^2) * BsmtQual_Num,
  )






data_clean <- data_clean %>%
  mutate(

  across(
    matches("(?i)(area|sf|sf1|sf2)$"), # Case-insensitive regex for "area" or "sf"
    ~ log1p(.),
    .names = "{.col}_LOG" # Append "Log" to the column name
  )
  # ,
  # across(
  #   matches("(?i)(area|sf|sf1|sf2)$"), # Case-insensitive regex for "area" or "sf"
  #   ~ ifelse(. == 0, NA, .),
  #   .names = "{.col}_Only" # Append "Log" to the column name
  # )
  )

#create polynomial features

data_clean <- data_clean %>% 
  mutate(
    TotalAreaSF_SQ = TotalAreaSF^2,
    TotalAreaSF_LOG_SQ = TotalAreaSF_LOG^2,
    TotalBsmt_SQ = TotalBsmtSF^2,
    FirstFloor_SQ = FirstFlrSF^2,
  )

data_clean <- data_clean %>%  dplyr::select(order(colnames(.))) # Order columns by name ascending

return(data_clean)

}

preprocess_data_with_response <- function(data_t, orderFactors = FALSE){

  data_t <- data
  data_clean <- preprocess_data(data_t, orderFactors)
    
  data_clean <- data_clean %>% 
  mutate(
    SalePriceLog = log(SalePrice),
    SalePrice = log(SalePrice)
  )
  
  return(data_clean)
  
}


```

## General EDA

Load the data

```{r}

library(data.table)
library(dplyr)

data = fread('data/train.csv')

testData <- fread('data/test.csv')

```

Create a visual GUI to browse the data at will.

```{r}

library(explore)

explore(data)


```

Create an automatic EDA report

```{r}

library(DataExplorer)

DataExplorer::create_report(data, output_file = "Data_Explorer_EDA.html")

```

The EDA report shows some missing data, so let's dive into that deeper.

```{r}

library(naniar)

missings <- data %>% 
  select(where(~ any(is.na(.))))

gg_miss_var(missings)
gg_miss_var(missings, show_pct = TRUE)

```



### Visual Plots

Look at plots of all vars by the SalesPrice

```{r}

library(ggplot2)
library(dplyr)
library(DataExplorer)
library(plotly)


data_all <- preprocess_data_with_response(data)

data_all <- data_all %>% filter(!(Id %in% c(1299,314,524, 250, 336, 739, 1374, 4997,1045,530,1025,1062,1356,582,452,707,534,1045,1374,441,497,333)))



#data_c$group1 <- ifelse(data_c$Id %in% c(376,1422, 810, 1182, 599, 956, 198, 1385, 1170, 399), 1, 0)

# Assuming your dataset is `data` and response variable is `response`
numeric_vars <- data_all %>%
  dplyr::select(where(is.numeric)) %>%
  dplyr::select(-SalePrice, -SalePriceLog) # Exclude the response variable from predictors

# Create scatter plots for numeric predictors
for (var in names(numeric_vars)) {
  p <- ggplot(data_all, aes_string(x = var, y = "SalePriceLog", color = data_all$Q, text = data_all$Id)) +
    geom_point() +
    #geom_smooth(method = "loess", se = FALSE, color = "blue") +
    #geom_smooth(method = "lm", se = FALSE, color = "red") +
    #geom_text(aes_string(label = "Id"), vjust = -0.5, size = 3, color = "red") + 
    labs(title = paste("Scatter Plot of", var, "vs SalePriceLog"),
         x = var, y = "SalePriceLog") +
    theme_minimal() +
    facet_grid(vars(data_all$NeighborhoodClass))
  
  # p2 <- ggplot(data_clean, aes_string(x = var, y = "SalePrice")) +
  #   geom_boxplot(outliers = TRUE, outlier.colour = "RED", outlier.shape = 2) +
  #   labs(title = paste("Scatter Plot of", var, "vs Response"),
  #        x = var, y = "Response") +
  #   theme_minimal()
  # 
  # print(p2)
  pl <- ggplotly(p)
  print(pl)
}

# catagorical_vars <- data_all %>%
#   dplyr::select(where(is.factor))
                
# for (var in names(catagorical_vars)) {
#   
#    p <- ggplot(data_all, aes_string(x = var, y = "SalePriceLog")) +
#     geom_boxplot(outliers = TRUE, outlier.colour = "RED", outlier.shape = 2) +
#     labs(title = paste("Box Plot of", var, "vs SalePriceLog"),
#          x = var, y = "SalePriceLog") +
#     theme_minimal()
#   
#   print(p)
# }



```
### Outliers

```{r}

library(outliers)

most_extreme <-outlier(data_all$GarageArea)

predictors <- names(data_all %>% select(-SalePriceLog, -SalePrice, -Id))

outlier_list <- list()
for (var in predictors) {
  mod <- lm(SalePrice ~ data_all[[var]], data = data_all)
  res <- resid(mod)
  cutoff <- quantile(abs(res), 0.995)  # top 0.5% as outliers
  flagged <- which(abs(res) > cutoff)
  outlier_list[[var]] <- flagged
}

numeric_vars <- names(data_all)[sapply(data_all, is.numeric) & names(data_all) != "SalePriceLog"]

# Define cutoff thresholds
lower_cutoff <- 0.005  # e.g., bottom 0.5%
upper_cutoff <- 0.995  # e.g., top 0.5%

# Group by Neighborhood and determine outliers for each numeric variable
data_outliers <- data_all %>%
  group_by(NeighborhoodClass) %>%
  do({
    df <- .
    # For each numeric variable, determine which rows are outliers
    outlier_flags <- lapply(numeric_vars, function(var) {
      vals <- df[[var]]
      # Compute quantiles for the current neighborhood
      low_bound <- quantile(vals, lower_cutoff, na.rm = TRUE)
      high_bound <- quantile(vals, upper_cutoff, na.rm = TRUE)
      # Flag rows that are outliers
      (vals < low_bound) | (vals > high_bound)
    })
    
    # Convert list of logical vectors to a data frame
    outlier_df <- as.data.frame(outlier_flags)
    names(outlier_df) <- paste0(numeric_vars, "_Outlier")
    
    # Bind the outlier flags back to the original data
    bind_cols(df, outlier_df)
  }) %>%
  ungroup()


```


### Thoughts on variables

From a visual analysis, it looks to me that the following variables are likely to be important in predicting the SalePrice of a house:

#### Continuous Variables

- GrLivArea
- GarageArea
- LotArea
- MasVnrArea
- SecondFlrSF
- TotalBsmtSF
- TotalRmsAbvGrd
- Year Built
- YearRemodalAdd

#### Categorical Variables

- HasGarage
- HasFirePlace
- HasVnr
- HasSecondFlr
- HasBsmt
- BsmtCondition
- HasPool
- PoolQC
- MassVnType
- Neighborhood
- OverallCondition
- OverallQuality
- Street

#### Potential 2-way Interactions

- GrLivArea:Neighborhood
- HasPool:PoolQC
- HasGarage:GarageArea
- HasVnr:MasVnrArea
- HasBsmt:TotalBsmtSF
- HasSecondFlr:SecondFlrSF

#### Potential 3-way Interactions

Decided against using these as there are collinearity problems that I wasn't able to address fully and ran out of time. 

- HasVnr:MasVnrArea:MassVnType
- HasBsmt:TotalBsmtSF:BsmtCondition

## ANALYSIS 1:

**Assume that Century 21 Ames (a real estate company) in Ames Iowa has commissioned you to answer a very important question with respect to their business. Century 21 Ames only sells houses in the NAmes, Edwards and BrkSide neighborhoods and would like to simply get an estimate of how the SalePrice of the house is related to the square footage of the living area of the house (GrLIvArea) and if the SalesPrice (and its relationship to square footage) depends on which neighborhood the house is located in. Build and fit a model that will answer this question, keeping in mind that realtors per to talk about living area in increments of 100 sq. ft. Provide your client with the estimate (or estimates if it varies by neighborhood) as well as confidence intervals for any estimate(s) you provide. It turns out that Century 21’s leadership team has a member that has some statistical background. Theore, make sure and provide evidence that the model assumptions are met and that any suspicious observations (outliers / influential observations) have been identified and addressed. Finally, of course, provide your client with a well written conclusion that quantifies the relationship between living area and sale price with respect to these three neighborhoods. Remember that the company is only concerned with the three neighborhoods they sell in.**

Filter and select the relevant data, then look at assumptions. From initial observatsions on the raw data, there does seem to be a general positive, linear relationship between GrLiveArea and SalePrice. However, the SalePrice data look to be right skewed, particularly in the Edwards and NAmes neighborhoods. Additionally, with the Edwards neighborhood, there seem to be several highly influential and high leverage points. Will readdress these points with a log of SalesPrice in the next code block.

```{r}
library(ggplot2)
library(GGally)

unique(data$Neighborhood)

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide')) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood) %>% 
  dplyr::mutate(
    Neighborhood = as.factor(Neighborhood)
  )

ggpairs(data_filtered)


```

The log transformation of SalePrice has helped a significant amount to settle down outliers and improve normality, however, there do seem to still be two significant outliers in the Edwards neighborhood. NOTE: address this later?

```{r}


data_filtered <- data_filtered %>% 
  mutate(
    SalePrice = log(SalePrice)
  )

ggpairs(data_filtered)


```

For now, keeping outliers in.

As expected, m0 has smallest adjusted R2, not taking into account Neighborhood at all. M1 improves upon M0. Interestingly M2 and M3 have the same adjusted R2, implying that the increased complexity of M3 does not help the model at all and can be left out. M4 and M5 both improve over M2 and M3 and also have the same adjusted R2. Similarly, this suggests that the added complexity of M5 over M4 is unnecessary.

```{r}

library(stringr)

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)




str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

```

Nested Model Significance testing shows exactly what we thought it would above.

```{r}

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

```

#### Log-Linear. - USING THIS ONE

Let's take a closer look at the outiers in the Edwards Neighborhood. Properties with ID'S 1299 and 524 are much larger than all the others in the neighborhood, by at least 2x. Unlikely that this is a data recording error, but possible. By removing these two data points, increase the adjusted R2 of our best model (M4) by 8%. In keeping with "all models are wrong but some are useful", it may be presenting this "wrong" model to the company with the caveat that it should only be used with houses 3000 sq ft or below. Very likely the improvement in accuracy is worth this trade off as there does large houses seem to be very rare. This sample suggests 2 out of \~380.

```{r}

data_all <- preprocess_data_with_response(data, orderFactors = FALSE)

data_filtered <- data_all %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide') & !(Id %in% c(1299,524))) %>% 
  dplyr::select(SalePriceLog, GrLivArea, Neighborhood, Id) #%>% 
  # dplyr::mutate(
  #   Neighborhood = as.factor(Neighborhood),
  #   #SalePrice = log(SalePrice)
  # )

data_filtered %>% ggplot(aes(x=GrLivArea, y=SalePriceLog, colour = Neighborhood)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(vars(Neighborhood))

#baseline, without neighborhood
m0 = lm(SalePriceLog ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePriceLog ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePriceLog ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePriceLog ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePriceLog ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePriceLog ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)


str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

summary(m4)

generate_diagnosticPlots(m4)
generate_model_stats(m4, data_filtered)

summary(m4)
confint(m4)

```

#### Log-Log

For completeness sake, we can log transform the GrLivArea. This transformation results in the same best model of M4, with practically the same adjusted R2 of .52. In keeping with the idea of parsimony, the easy of interpretation with the log-linear model without outliers, may be more practical than this log-log model with those outliers included.

```{r}

library(ggplot2)

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide') & !(Id %in% c(1299,524))) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood, Id) %>% 
  dplyr::mutate(
    Neighborhood = as.factor(Neighborhood),
    GrLivArea = log(GrLivArea),
    SalePrice = log(SalePrice)
  )

data_filtered %>% ggplot(aes(x=GrLivArea, y=SalePrice, colour = Neighborhood)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(vars(Neighborhood))

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)

#GrLiveArea and Neighborhood effect + Interaction
m6 = lm(SalePrice ~ GrLivArea * Neighborhood, data = data_filtered)
s6 = summary(m6)


str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

generate_diagnosticPlots(m6)

generate_model_stats(m6, data_filtered)
```

#### Linear 

Often common to jump to log of money data, but let's check the linear model with no transformations. We can see from the diagnostic plots, that we there is evidence of moderate increasing variance as well as a right skewed histogram of the residuals. These problems are improved upon when the log of the sales price is used.

```{r}

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide') & !(Id %in% c(1299,524))) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood, Id)

data_filtered %>% ggplot(aes(x=GrLivArea, y=SalePrice, colour = Neighborhood)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(vars(Neighborhood))

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)


str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

generate_diagnosticPlots(m4)

generate_model_stats(m4, data_filtered)
```

## ANALYSIS 2

**Build the most predictive model for sales prices of homes in all of Ames, Iowa.  This includes all neighborhoods. Your group is limited to only the techniques we have learned in 6371 (no random forests or other methods we have not yet covered).  Specifically, you should produce at least 2 competing models: a simple linear regression model (you pick the explanatory variable) and a multiple linear regression model (SalePrice\~GrLivArea + FullBath) and at least one additional multiple linear regression model where you select the explanatory variables.  Generate an adjusted R^2^, CV Press and Kaggle Score for each of these models and clearly describe which model you feel is the best in terms of being able to predict future sale prices of homes in Ames, Iowa.  In your paper, please include a table similar to the one below.  The group with the lowest public Kaggle score will receive an extra 3 bonus points on the final exam!** 


#### Simple Linear Regression Model

To find the best simple linear model, we loop through all the continuous variables and fit a model. To perhaps no surprise, the best model was GrLivArea. Since we already fit this model in analysis one, we moved to the next highest adj r2, which was GarageCars. This model had debatable diagnostic plots, so we moved to the next highest which was GarageArea. This model fits the diagnostic plots very well, except for 4 outliers where the garage size is abnormally large. Like GrlLivArea, we will suggest the use of this model in a range. In this instance, between 0 and 1250 sq ft.

```{r}
library(stringr)
library(car)
library(ggplot2)

data_continuous <- data_all %>% select_if(is.numeric)

response <- data_continuous$SalePrice
predictors <- data_continuous %>% select(-all_of(c('SalePrice','GrLivArea', 'GarageCars')))


formulas1 <- foreach(predictor = names(predictors)) %do%{
  f <- str_glue("SalePrice~{predictor}")
  as.formula(f)
}

formulas2 <- foreach(predictor = names(predictors)) %do%{
  f <- str_glue("SalePrice~log({predictor} + .00001)")
  as.formula(f)
}

formulas <- c(formulas1, formulas2)

results <- foreach(formula = formulas) %do% {
  m = lm(formula, data = data_all)
  summary(m)
}

adj_r2 <- sapply(results, function(res) res$adj.r.squared)

best_index <- which.max(adj_r2)

f <- formulas[[best_index]]

print(f)

results[[best_index]]

```

Diagnostic Plots for GarageArea

```{r}

data_garage <- data_all %>% select(SalePrice, GarageArea)

ggplot(data_garage, aes(x=GarageArea, y=SalePrice)) + geom_point() + geom_smooth(method = 'lm')

m = lm(f, data = data_all)

generate_diagnosticPlots(m)

```

##### M1. - USE THIS ONE

Model Statistics and Predictions

- Kaggle Score: 0.31527

```{r}

library(caret)
library(doParallel)


data_all <- preprocess_data_with_response(data)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

M <- lm(SalePriceLog ~ GarageArea, data = data_all) 

model <- generate_model_stats(M, data_all)
generate_diagnosticPlots(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_SLR.csv", row.names = FALSE)

```

#### Multiple Linear Regression - Feature Analysis/Selection

##### Continuous Variable Analysis

Looking for multi-collinearity with CAR package. It would seem GrLivArea has perfect collineratity with several other variables, which we will remove.

For GrLivArea: FirstFlrSF, SecondFlrSF, LowQualFinSF

For TotalBsmtSF: BsmtFinSF1, BsmtFinSF2, BsmtUnfSF

```{r}

library(car)

data_continuous <- data_all %>% select_if(is.numeric)
base_model <- lm(SalePrice ~., data = data_continuous)
alias(SalePrice ~., data = data_continuous)

col_to_remove <- c('FirstFlrSF','SecondFlrSF','LowQualFinSF','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF')

data_continuous_clean <- data_continuous %>% dplyr::select(-all_of(col_to_remove))

base_model_clean <- lm(SalePrice ~., data = data_continuous_clean)
vifs <- vif(base_model_clean)

print(vifs)

# # Compute correlation matrix
# cor_matrix <- cor(data_continuous_clean, use = "pairwise.complete.obs")
# 
# # Extract correlations with GrLivArea
# grlivarea_cor <- cor_matrix["GrLivArea", ]
# 
# # Sort correlations in decreasing order
# grlivarea_cor_sorted <- sort(abs(grlivarea_cor), decreasing = TRUE)
# 
# print(grlivarea_cor_sorted)
```


##### Categorical Variable Analysis

Not sure this was helpful as it seems to suggest everything is statistically relevant. Though maybe it would be helpful to take the most relevant ones.

Neighborhood, ExterQual, KitchenQual, Foundation, GarageFinish, BsmtQual, OverallQual, GarageType

```{r}

data_cata <- data_all %>% select_if(is.factor)

print('ANOVA TESTS')
for (var in names(data_cata)) {
  formula <- as.formula(paste("SalePrice ~", var))
  anova_result <- aov(formula, data = data_all)
  p_value <- summary(anova_result)[[1]][["Pr(>F)"]][1]
  if (p_value < 0.05) {
    cat("Variable:", var, "- p-value:", p_value, "\n")
  }
}


print('KRUSKAL-WALLIS TEST')
for (var in names(data_cata)) {
  formula <- as.formula(paste("SalePrice ~", var))
  kruskal_result <- kruskal.test(formula, data = data_all)
  p_value <- kruskal_result$p.value
  cat("Variable:", var, "- p-value:", p_value, "\n")
}


```

##### Combined/Advanced Feature Selection Techniques

MASS library can target AIC by doing stepwise feature selection. Using "Both" in this instance. This will be one model that we test in the paper.

```{r}

library(MASS)

model <- lm(SalePrice ~ ., data = data_all)
set.seed(100)
step_model <- stepAIC(model, direction = "both", trace = FALSE)

```

Limited Leaps as a full run would take too long. This particular run, the same model had the highest CP, BIC, and ADJR2, which was:

YearBuilt, YearRemoAdd, HeatingGasW, HalfBath, GarageQualPo

```{r}

library(leaps)

fit <- regsubsets(SalePrice ~ ., data = data_all, nvmax = 4, nbest = 5, method = 'exhaustive', really.big = TRUE)

fit_sum <- summary(fit)

# Identify the best models according to different metrics
best_adjr2_index <- which.max(fit_sum$adjr2)
best_cp_index <- which.min(fit_sum$cp)
best_bic_index <- which.min(fit_sum$bic)

# Extract coefficients
best_adjr2_model <- coef(fit, id = best_adjr2_index)
best_cp_model <- coef(fit, id = best_cp_index)
best_bic_model <- coef(fit, id = best_bic_index)

print(best_adjr2_model)
print(best_cp_model)
print(best_bic_model)


#par(mfrow = c(1, 2))
#plot(fit, scale = "adjr2", main = "Adjusted R-squared")
```


After removing perfect collinearity, vif() was able to run. Some research seems to suggest that a VIF of greater than 5 show's high correlation to other variables in the mode. I suspect that garagecars and garagearea are related, so let's try removing one of those and checking vif again.

Let's take the continuous columns we found in the VIF analysis and then the most significant categorical variables we found. Can't use leaps on catagorical vars, so trying olsrr.

```{r}

library(olsrr)
library(dplyr)
library(car)

data_continuous <- data_all %>% select_if(is.numeric)

col_to_remove <- c('FirstFlrSF','SecondFlrSF','LowQualFinSF','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'GarageArea')
data_continuous_clean_t1 <- data_continuous %>% dplyr::select(-all_of(col_to_remove))
base_model_clean_t1<- lm(SalePrice ~., data = data_continuous_clean_t1)
vifs_t1 <- vif(base_model_clean_t1) 
print(vifs_t1)

sig_cata <- c('Neighborhood', 'ExterQual', 'KitchenQual', 'Foundation', 'GarageFinish', 'BsmtQual', 'OverallQual', 'GarageType')

continuous_cols <- data_all %>% select(all_of(names(vifs_t1)))
cata_cols <- data_all %>% select(all_of(sig_cata))

candidate_data <- cbind(continuous_cols, cata_cols, SalePrice = data_all$SalePrice)

#model_mat <- model.matrix(~ ., data = candidate_data)[, -1]

model <- lm(SalePrice ~ ., data = candidate_data)

set.seed(100)
fit_forward <- ols_step_forward_adj_r2(model)

set.seed(100)
fit_backward <- ols_step_backward_adj_r2(model)

set.seed(100)
fit_both <- ols_step_both_adj_r2(model)

set.seed(100)
fit_heir <- ols_step_both_p(model, .05, heirarchical = TRUE)

saveRDS(fit_forward, 'models/forward_fit.rds')
saveRDS(fit_both, 'models/fit_both.rds')
saveRDS(fit_heir, 'models/fit_heir.rds')


# model <- lm(SalePrice ~ ., data = data_all)
# 
# set.seed(100)
# fit_forward <- ols_step_forward_adj_r2(model)
# 
# set.seed(100)
# fit_backward <- ols_step_backward_adj_r2(model)
# 
# set.seed(100)
# fit_both <- ols_step_both_adj_r2(model)
# 
# set.seed(100)
# fit_heir <- ols_step_both_p(model, .05, heirarchical = TRUE)
# 
# saveRDS(fit_forward, 'models/forward_fit_all.rds')
# saveRDS(fit_both, 'models/fit_both_all.rds')
# saveRDS(fit_heir, 'models/fit_heir_all.rds')



```

Research suggested using Boruta as a way to suggest features. This did not seem to help narrow it down at all, but to be fair, we haven't studied this and I'm not entirely sure what it is doing, but was hopeful it might point in the right direction. The idea was that if x number of selection techniques generally agreed, that would be a good sign.

```{r}

library(Boruta)

set.seed(100)

boruta_out <- Boruta(SalePrice ~ ., data = data_all, doTrace = 0)

final_vars_tenative <- getSelectedAttributes(boruta_out, withTentative = TRUE)
final_vars <- getSelectedAttributes(boruta_out, withTentative = FALSE)

print('finals')
print(final_vars)

print('finals-with tentative')
print(final_vars)

```

LASSO keeps coming up in my research for data sets with a large number of features. This look cool, not again, not sure what exactly it's doing as we haven't studied it.

```{r}

library(glmnet)

x <- model.matrix(SalePrice ~ ., data = data_all)[, -1]
y <- data_all$SalePrice

lasso_cv <- cv.glmnet(x, y, alpha = 1)

optimal_lambda <- lasso_cv$lambda.min

lasso_model <- glmnet(x, y, alpha = 1, lambda = optimal_lambda)

lasso_coefficients <- coef(lasso_model)

print(lasso_coefficients)

```

#### Multiple Linear Regression - Models

##### M0 - USE THIS ONE - MANDATORY?

- Kaggle Score: 0.29097

```{r}

library(caret)
library(doParallel)

data_all <- preprocess_data_with_response(data, orderFactors = FALSE)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

M <- lm(SalePriceLog ~ GrLivArea + FullBath, data = data_all)

model <- generate_model_stats(M, data_all)
generate_diagnosticPlots(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M0.csv", row.names = FALSE)


```

##### M1 - Use this one

- Kaggle Score: 0.14395

Found by MASS package, targeting AIC. Seems somewhat problematic perhaps. Histogram of residuals looks great and the residual scatter plot is decent, but the QQ plot looks mildly concerning as well as the leverage plot. Oddly,

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/potential_model_1.rds')

data_all <- preprocess_data_with_response(data, FALSE)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

model <- generate_model_stats(M$model, data_all)
generate_diagnosticPlots(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M1.csv", row.names = FALSE)


```


##### M2

Found by olsrr package, targeting adjr2 with forward selection on subjectively selected data

- Kaggle Score: 0.14914

```{r}

library(caret)
library(doParallel)

data_all <- preprocess_data_with_response(data)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

M <- readRDS('models/forward_fit.rds')

model <- generate_model_stats(M$model, data_all)
#generate_diagnosticPlots(m$finalModel)

#ols_plot_cooksd_bar(model$finalModel)
ols_plot_diagnostics(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M2.csv", row.names = FALSE)

```

##### M3 - Use this one

Found by olsrr package, targeting adjr2 with both selection on subjectively selected data. 

- Kaggle Score: 0.14914   

Need to check if this is the same mode as above, since the kaggle scores were the same. 

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/fit_both.rds')

data_all <- preprocess_data_with_response(data)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

model <- generate_model_stats(M$model, data_all)
#generate_diagnosticPlots(m$finalModel)

#ols_plot_cooksd_bar(model$finalModel)
ols_plot_diagnostics(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M3.csv", row.names = FALSE)


```

##### M4

Found by olsrr package, targeting adjr2 with both heir selection on subjectively selected data

- Kaggle Score: 0.14914

```{r}

library(caret)
library(doParallel)


M <- readRDS('models/fit_heir.rds')

data_all <- preprocess_data_with_response(data)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

model <- generate_model_stats(M$model, data_all)
generate_diagnosticPlots(m$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M4.csv", row.names = FALSE)

```

##### M5 

Let's look at the variables that we found in the visual analysis. 

- M5.1 - Kaggle - Score: 0.14668
- M5.2 - Kaggle - Score: 0.14665

```{r}

library(caret)
library(parallel)
library(olsrr)

#calling this M5.1

data_all <- preprocess_data_with_response(data)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,524)))

M <- lm(SalePriceLog ~ 
              GrLivArea + GarageArea + LotArea + MasVnrArea + SecondFlrSF + TotalBsmtSF + TotRmsAbvGrd + YearBuilt + YearRemodAdd +
              HasGarage + HasFireplace + HasVnr + HasSecondFloor + HasBsmt + BsmtCond + HasPool + PoolQC + MasVnrType + Neighborhood + OverallCond + OverallQual + Street + 
              GrLivArea:Neighborhood + HasPool:PoolQC + HasGarage:GarageArea + HasVnr:MasVnrArea + HasBsmt:TotalBsmtSF + HasSecondFloor:SecondFlrSF +
              HasVnr:MasVnrArea:MasVnrType + HasBsmt:TotalBsmtSF:BsmtCond
              , data = data_all)

model <- generate_model_stats(M, data_all)
generate_diagnosticPlots(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M5_normaldata.csv", row.names = FALSE)


#M5.2 - removing outliers


data_all_minus_outliers <- data_all %>% filter(!(Id %in% c(1299, 1424, 235, 530, 523)))

M <- lm(SalePriceLog ~ 
              GrLivArea + GarageArea + LotArea + MasVnrArea + SecondFlrSF + TotalBsmtSF + TotRmsAbvGrd + YearBuilt + YearRemodAdd +
              HasGarage + HasFireplace + HasVnr + HasSecondFloor + HasBsmt + BsmtCond + HasPool + PoolQC + MasVnrType + Neighborhood + OverallCond + OverallQual + Street + 
              GrLivArea:Neighborhood + HasPool:PoolQC + HasGarage:GarageArea + HasVnr:MasVnrArea + HasBsmt:TotalBsmtSF + HasSecondFloor:SecondFlrSF +
              HasVnr:MasVnrArea:MasVnrType + HasBsmt:TotalBsmtSF:BsmtCond
              , data = data_all_minus_outliers)


model <- generate_model_stats(M, data_all_minus_outliers)
generate_diagnosticPlots(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,           
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M5_minus_outliers.csv", row.names = FALSE)

```

##### M6

```{r}

library(doParallel)
library(caret)
library(dplyr)
library(stringr)
library(olsrr)
library(car)

data_all <- preprocess_data_with_response(data)
test_data <- preprocess_data(testData)

data_all <- data_all %>% filter(!(Id %in% c(1299,314,524, 250, 336, 739, 1374, 4997,1045,530,1025,1062,1356,582,452,707,534,1045,1374,441,497,333)))
#data_all <- data_all %>% filter(!(Id %in% c(1299,314,524, 250, 336, 739, 1374, 4997,1045,530,1025,1062,1356,582,452,707,534,1045,1374,441,497,333, 999, 988, 407, 578, 89)))

zero_indices <- which(data_all$SecondFlrSF == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_all$SecondFlrSF[random_index] <- 0.1
}

zero_indices <- which(data_all$SecondFlrSF_LOG == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_all$SecondFlrSF_LOG[random_index] <- 0.1
}


zero_indices <- which(data_all$TotalBsmtSF == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_all$TotalBsmtSF[random_index] <- 0.1
}

zero_indices <- which(data_all$TotalBsmtSF_LOG == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_all$TotalBsmtSF_LOG[random_index] <- 0.1
}

zero_indices <- which(data_all$GarageYrBlt == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_all$GarageYrBlt[random_index] <- 0.1
}

# M <- lm(SalePriceLog ~ 
#               GrLivArea_LOG + LotArea_LOG + MasVnrArea_LOG + SecondFlrSF_LOG + TotalBsmtSF_LOG + TotRmsAbvGrd + YearBuilt + YearRemodAdd +
#               HasFireplace +  BsmtCond + PoolQC + MasVnrType + HasGarage + Neighborhood + OverallCond + OverallQual + Street + 
#               GrLivArea_LOG:Neighborhood + HasBsmt:TotalBsmtSF_LOG + HasSecondFloor:SecondFlrSF_LOG 
#               , data = data_all)



# M <- lm(SalePriceLog ~ 
#               LotArea_LOG + MasVnrArea_LOG + + TotalBaths + YearsSinceRemodel +
#               HasFireplace +  HasGarage + OverallQual + GarageCars +
#               TotalAreaSF_LOG:NeighborhoodClass + 
#                 OverallQual:GrLivArea_LOG
#               , data = data_all)

# M <- lm(
#   SalePriceLog ~
#     BsmtExposure_Num +
#     BsmtFinSF1 +
#     BsmtFinSF2 +
#     BsmtFinSF2_LOG +
#     BsmtFinType1_Num +
#     BsmtQual_Num +
#     CentralAir +
#     Condition1 +
#     EnclosedPorch +
#     ExterCond_Num +
#     Exterior1st +
#     Fireplaces +
#     Foundation +
#     Functional_Num +
#     GarageArea_LOG +
#     HalfBath +
#     Heating +
#     HeatingQC_Num +
#     KitchenAbvGr +
#     KitchenQual_Num +
#     LandSlope +
#     LotArea_LOG +
#     LotConfig +
#     MasVnrType +
#     MSZoning +
#     Neighborhood +
#     OpenPorchSF +
#     OverallCond +
#     OverallQual +
#     PoolQC_Num +
#     SaleCondition +
#     SaleType +
#     ScreenPorch +
#     Street +
#     ThirdSsnPorch +
#     TotalAreaSF_LOG +
#     TotalAreaSF_SQ +
#     TotalBaths +
#     Utilities +
#     WoodDeckSF +
#     YearBuilt +
#     YearRemodAdd,
#   data = data_all
# )

#kaggle .13482
M <- lm(
  SalePriceLog ~
    Fireplaces +
    GarageArea_LOG +
    KitchenQual_Num +
    LotArea_LOG +
    OverallCond +
    OverallQual +
    TotalAreaSF_SQ +
    TotalBsmt_SQ +
    TotalBaths +
    FirstFloor_SQ +
    WoodDeckSF +
    YearBuilt +
    YearRemodAdd +
    QualityTotalSF:NeighborhoodClass_Num +
    QualityTotalSF_LOG:NeighborhoodClass_Num +
    BsmtFinSF1:NeighborhoodClass_Num +
    TotalAreaSF_LOG_SQ:NeighborhoodClass_Num ,

  data = data_all
)

model <- generate_model_stats(M , data_all)
generate_diagnosticPlots(model$finalModel)

vif(model$finalModel)

predictions <- predict(model, newdata = test_data)

results <- data.frame(
  Id = test_data$Id,
  PredictionsLog = predictions,
  SalePrice = exp(predictions)
)

results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M6.1.1_minus_outliers.csv", row.names = FALSE)


# t1 = ols_step_both_adj_r2(M)
# t2 = ols_step_both_aic(M)
# t3 = ols_step_both_sbc(M)
# t4 = ols_step_both_sbic(M)
# t5 = ols_step_both_p(M)
# 
# model_t1 <- generate_model_stats(t1$model, data_all)
# model_t2 <- generate_model_stats(t2$model, data_all)
# model_t3 <- generate_model_stats(t3$model, data_all)
# model_t4 <- generate_model_stats(t4$model, data_all)
# model_t5 <- generate_model_stats(t5$model, data_all)
# 
# generate_diagnosticPlots(model_t1$finalModel)
# generate_diagnosticPlots(model_t2$finalModel)
# generate_diagnosticPlots(model_t3$finalModel)
# generate_diagnosticPlots(model_t4$finalModel)
# generate_diagnosticPlots(model_t5$finalModel)
# 
# ols_plot_added_variable(model_t1$finalModel)


# predictions <- predict(model, newdata = test_data)
# 
# results <- data.frame(
#   Id = test_data$Id,           
#   PredictionsLog = predictions,
#   SalePrice = exp(predictions)
# )
# 
# results %>% select(Id, SalePrice) %>% write.csv("predictions/results_analysis_2_MLR_M6_minus_outliers.csv", row.names = FALSE)

# data_q <- data_all %>% filter(Id %in% c(376,1422, 810, 1182, 599, 956, 198, 1385, 1170, 399, 917, 810))
# 
# residuals <- rstudent(model$finalModel)  # Studentized residuals
# leverage <- hatvalues(model$finalModel)
# 
# # Thresholds
# high_residuals <- which(residuals < -2)  # Left-skewed problematic points
# high_leverage <- which(leverage > (2 * mean(leverage)))  # High leverage points
# 
# # Combine and check indices
# problem_points <- intersect(high_residuals, high_leverage)
# print(problem_points)
# 
# # Plot diagnostic points
# plot(residuals, leverage, main = "Residuals vs Leverage")
# points(high_residuals, leverage[high_residuals], col = "red", pch = 19)
# 
# 
# data_all <- data_all %>% filter(!(Id %in% c(1299,524, 250, 314, 666, 812)))
# 
# par(mfrow = c(2, 2))
# for (var in names(data_c)[sapply(data_c, is.numeric)]) {
#   hist(data_c[[var]], main = paste("Histogram of", var), xlab = var)
# }
# 
# max_row <- data_all %>%
#   filter(HasBsmt == 'Yes') %>%
#   filter(SalePriceLog == min(SalePriceLog, na.rm = TRUE))
# 
# print(max_row)

```


Model analysis stuff


```{r}

library(corrplot)

#l <- lm(SalePriceLog ~ ., data = data_all %>% select(-Id, -SalePrice))
#r <- ols_step_both_aic(l)

#saveRDS(r$model, 'models/potential_m6.rds')

outlierTest(model$finalModel)

ols_plot_added_variable(model$finalModel)

vif(model$finalModel)

model_vars <- all.vars(terms(model$results))

# Subset the data with those variables
model_data <- data_all[, model_vars]

numeric_model_data <- model_data[, sapply(model_data, is.numeric)]

# Compute correlation matrix
cor_matrix <- cor(numeric_model_data, use = "pairwise.complete.obs")

corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black")


# paste(model_vars, collapse = " + ")
# 
# 
# m <- lm(SalePriceLog ~ OverallQual + Neighborhood + OverallCond + YearBuilt + LotArea_LOG + MSZoning + Functional + SaleCondition + KitchenQual + Condition1 + HeatingQC_Num + BsmtExposure + Exterior1st + TotalAreaSF_LOG + ScreenPorch + YearRemodAdd + Fireplaces + Foundation + PoolQC + SaleType + BsmtQual + TotalBaths + CentralAir + GrLivArea + BsmtFinType1_Num +  Heating + LotConfig + WoodDeckSF + ExterCond + MasVnrType + TotalAreaSF_SQ + LandSlope + HalfBath + BsmtFinSF1 + BsmtFinSF2 + GarageArea_LOG + EnclosedPorch + KitchenAbvGr + Street + BsmtFinSF2_LOG + OpenPorchSF + ThirdSsnPorch + Utilities, data = data_all)
# 
# vif(m)



```

###### RFE

```{r}

library(ggplot2)
library(dplyr)
library(tidyr)  # For pivot_longer

# Function to visualize SalePriceLog by Neighborhood
plot_neighborhood_analysis <- function(data, price_var = "", neighborhood_var = "Neighborhood") {
  
  # 1. Boxplot of SalePriceLog by Neighborhood
  p1 <- ggplot(data, aes_string(x = neighborhood_var, y = price_var)) +
    geom_boxplot(fill = "lightblue", color = "black", outlier.color = "red") +
    labs(title = "Boxplot of SalePriceLog by Neighborhood",
         x = "Neighborhood", y = "SalePriceLog") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
  
  print(p1)
  
  # 2. Mean and Median SalePriceLog for each neighborhood
  summary_stats <- data %>%
    group_by(!!sym(neighborhood_var)) %>%
    summarise(
      Mean = mean(!!sym(price_var), na.rm = TRUE),
      Median = median(!!sym(price_var), na.rm = TRUE)
    ) %>%
    pivot_longer(cols = c("Mean", "Median"), names_to = "Statistic", values_to = "Value")
  
  # Bar chart of Mean and Median SalePriceLog
  p2 <- ggplot(summary_stats, aes(x = reorder(!!sym(neighborhood_var), -Value), y = Value, fill = Statistic)) +
    geom_bar(stat = "identity", position = "dodge") +
    labs(title = "Mean and Median SalePriceLog by Neighborhood",
         x = "Neighborhood", y = "SalePriceLog") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
  
  print(p2)
  
  # Return summary stats for further analysis
  return(summary_stats)
}

# Example usage:
# Assuming 'data_all' is your dataset with 'SalePriceLog' and 'Neighborhood'
summary_stats <- plot_neighborhood_analysis(data_all %>% mutate(SalePrice = exp(SalePriceLog)), "SalePrice", "Neighborhood")

```



```{r}

data_all <- preprocess_data_with_response(data)

set.seed(100)

# Step 1: Custom summary function for RMSE
customSummary <- function(data, lev = NULL, model = NULL) {
  RMSE <- sqrt(mean((data$obs - data$pred)^2))
  c(RMSE = RMSE)  # Named list for RMSE
}

# Step 2: Extend caretFuncs to use custom summary function
customFuncs <- caretFuncs  # Clone caretFuncs
customFuncs$summary <- customSummary  # Replace the summary function

control <- rfeControl(functions = customFuncs,  # Use Linear Regression
                      method = "cv",       # Cross-validation
                      number = 10,         # 10-fold CV
                      verbose = TRUE)

predictors <- data_all %>% select(-SalePriceLog, -SalePrice)  # Remove the target variable
target <- data.frame(SalePriceLog = data_all$SalePriceLog)

# predictors <- data_all %>% select(where(is.numeric)) 
# data_new <- data.frame(predictors, target)

# Preprocess the data: scaling, centering, imputation, and dummy variables
preProcess_params <- preProcess(predictors, 
                                method = c("center", "scale"))

# Apply preprocessing to the predictors
predictors_processed <- predict(preProcess_params, predictors)

# Convert categorical variables to dummy variables
dummy_params <- dummyVars("~ .", data = predictors_processed)

# Apply dummy encoding
predictors_final <- predict(dummy_params, newdata = predictors_processed)

predictors_final <- as.data.frame(predictors_final) %>% select(where(is.numeric))

predictors_final <- predictors_final %>% select(where(is.numeric)) 

data_new <- cbind(predictors_final, target)

num_cores <- parallel::detectCores() - 2
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Run RFE
results <- rfe(SalePriceLog ~ ., data = data_new, rfeControl = control, metric = "RMSE")

stopCluster(cl)

# View results
print(results)

```


```{r}

# ANOVA Test

data_all <- preprocess_data_with_response(data)

anova_result <- aov(SalePriceLog ~ NeighborhoodClass, data = data_all)
summary(anova_result)

tukey_result <- TukeyHSD(anova_result)
print(tukey_result)


# Pairwise comparisons using ggpubr
library(ggpubr)

ggboxplot(data_all, x = "NeighborhoodClass", y = "SalePriceLog", 
          color = "Neighborhood") +
  stat_compare_means(method = "kruskal.test") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```


```{r}
library(dplyr)
data_c <- preprocess_data_with_response(data)
data_c$group1 <- ifelse(data_c$Id %in% c(376,1422, 810, 1182, 599, 956, 198, 1385, 1170, 399), 1, 0)


data_c <- data_c %>% select(-SalePriceLog, -SalePrice)

M <- lm(group1 ~ 
              GrLivArea + LotArea + MasVnrArea + SecondFlrSF + TotalBsmtSF + TotRmsAbvGrd + YearBuilt + YearRemodAdd +
              HasFireplace +  BsmtCond + PoolQC + HasGarage + MasVnrType + Neighborhood + OverallCond + OverallQual + Street + 
              GrLivArea:Neighborhood + HasBsmt:TotalBsmtSF + HasSecondFloor:SecondFlrSF
              , data = data_c)

library(FSelector)
library(scorecard)
library(rpart)

tree <- rpart(SalePriceLog ~ ., data = data_all %>% select(-SalePrice), method = "anova")
summary(tree)


library(randomForest)

data_all <- preprocess_data_with_response(data)

# Fit a random forest model (replace SalePrice with your target variable)
rf_model <- randomForest(SalePriceLog ~ ., data = data_all %>% select(-SalePrice), importance = TRUE, ntree = 500)

# View variable importance
importance(rf_model)  # Gini index or %IncMSE
varImpPlot(rf_model)  # Visualize importance



library(rpart)
library(rpart.plot)

# List of categorical variables
cat_vars <- names(data_all)[sapply(data_all, is.factor)]

# Function to bin categorical variables using decision trees
bin_with_tree <- function(data, cat_var, target) {
  tree <- rpart(as.formula(paste(target, "~", cat_var)), data = data, method = "anova")
  predictions <- predict(tree, data)
  
  # Group levels based on predicted values
  binned_levels <- cut(predictions, breaks = 3, labels = c("Low", "Medium", "High"))
  return(as.factor(binned_levels))
}

# Apply tree-based binning to all categorical variables
for (var in cat_vars) {
  new_var_name <- paste0(var, "_Binned")
  data_all[[new_var_name]] <- bin_with_tree(data_all, var, "SalePrice")
}

# Check the results
str(data_all)
```


Distributed Metric Checking? Just generating the list of all formulas takes forever... maybe easier to do on the wine data set? 

```{r}
library(combinat)
library(tidyverse)
library(Metrics)
library(dplyr)
library(magrittr)

# Function to compute metrics for a given formula
evaluate_model <- function(formula, data) {
  model <- lm(as.formula(formula), data = data)
  predictions <- predict(model, data)
  
  # Calculate metrics
  mae <- mae(data$SalePriceLog, predictions)
  rmse <- rmse(data$SalePriceLog, predictions)
  adj_r2 <- summary(model)$adj.r.squared
  aic <- AIC(model)
  bic <- BIC(model)
  mape <- mean(abs((data$SalePriceLog - predictions) / data$SalePriceLog)) * 100
  mallows_cp <- sum(residuals(model)^2) / (summary(model)$sigma^2) - (2 * (length(model$coefficients) - 1))
  
  return(data.frame(
    Formula = formula,
    Adj_R2 = adj_r2,
    MAE = mae,
    RMSE = rmse,
    AIC = aic,
    BIC = bic,
    MAPE = mape,
    Mallows_CP = mallows_cp
  ))
}

library(parallel)

# Create a function to generate formulas for a given combination size
generate_formulas <- function(k, predictors) {
  combn(predictors, k, simplify = FALSE) |>
    lapply(function(comb) paste("SalePriceLog ~", paste(comb, collapse = " + ")))
}

# Define the predictors
predictors <- c("GrLivArea", "GarageArea", "LotArea", "MasVnrArea", "SecondFlrSF", 
                "TotalBsmtSF", "TotRmsAbvGrd", "YearBuilt", "YearRemodAdd",
                "HasGarage", "HasFireplace", "HasVnr", "HasSecondFloor", 
                "HasBsmt", "BsmtCond", "HasPool", "PoolQC", "MasVnrType", 
                "Neighborhood", "OverallCond", "OverallQual", "Street", 
                "GrLivArea:Neighborhood", "HasPool:PoolQC", "HasGarage:GarageArea", 
                "HasVnr:MasVnrArea", "HasBsmt:TotalBsmtSF", "HasSecondFloor:SecondFlrSF")

# Set up parallel processing
cl <- makeCluster(detectCores() - 1) # Use all but one core
clusterExport(cl, c("predictors", "generate_formulas")) # Export objects to workers

# Generate formulas in parallel
formulas <- parLapply(cl, 1:length(predictors), function(k) {
  generate_formulas(k, predictors)
})

stopCluster(cl) # Stop the cluster

# Flatten list of lists
formulas <- do.call(c, formulas)

names(data_all)
```


```{r}

library(glmnet)

data_all <- preprocess_data_with_response(data)

# Prepare data
x <- model.matrix(SalePriceLog ~ ., data = data_all)[, -1]  # Remove intercept
y <- data_all$SalePriceLog

# Run Lasso Regression (alpha = 1 for Lasso)
set.seed(123)
lasso_model <- cv.glmnet(x, y, alpha = 1, family = "gaussian")

# Plot cross-validation results
plot(lasso_model)

# Coefficients at optimal lambda
coef(lasso_model, s = "lambda.min")


```

```{r}

# Load required libraries
library(mlr3)
library(mlr3fselect)  # Feature selection extension
library(mlr3learners) # Learners like linear regression
library(mlr3viz)      # For visualization
library(paradox)      # Parameter search space
library(mlr3pipelines)
library(mlr3filters)


# Near-Zero Variance Filter Operator
filter_nzv <- po("filter", filter = flt("variance"), param_vals = list(filter.cutoff = 0.01))

# Preprocessing pipeline: near-zero variance filtering, scaling, and one-hot encoding
preprocess_pipeline <- gunion(list(
  filter_nzv,                       # Remove near-zero variance features
  po("scale"),                      # Center and scale numeric features
  po("encode", method = "one-hot")  # Encode categorical features
))

# Step 1: Define the Task
# Assume 'SalePriceLog' is the target variable
task <- TaskRegr$new(id = "house_prices", backend = data_all, target = "SalePriceLog")

# Step 2: Define the Learner (Linear Regression)
learner <- lrn("regr.lm")

pipeline <- preprocess_pipeline %>% learner

# Step 3: Define the Resampling Strategy
resampling <- rsmp("cv", folds = 5)  # 5-fold cross-validation

# Step 4: Define the Feature Selection Terminator
# Stop after evaluating 50 feature subsets
terminator <- trm("evals", n_evals = 50)

# Step 5: Define the Feature Selection Strategy (RFE)
fselector <- FSelectorBatchSequential$new()

# Step 6: AutoFSelector (Automates feature selection process)
auto_fselector <- AutoFSelector$new(
  learner = learner,
  resampling = resampling,
  measure = msr("regr.rsq"),  # Use R-squared for evaluation
  terminator = terminator,
  fselector = fselector
)

# Step 7: Train AutoFSelector
set.seed(123)
auto_fselector$train(task)

# Step 8: View Results
# Selected features
print(auto_fselector$result_feature_set)

# Plot the feature selection process
autoplot(auto_fselector)
```

maybe we can loop through stepwise a few times at different seeds and see if there is a consensus.

```{r}

library(foreach)
library(parallel)
library(olsrr)
library(dplyr)
library(car)
library(doParallel)

data_testing <- data_all# %>% filter(!(Id %in% c(1299, 1424)))

zero_indices <- which(data_testing$SecondFlrSF == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_testing$SecondFlrSF[random_index] <- 0.1
}

zero_indices <- which(data_testing$TotalBsmtSF == 0)

# Check if there are any rows with 0
if (length(zero_indices) > 0) {
  # Randomly select one index from the rows where SecondFlrSF is 0
  set.seed(100)
  random_index <- sample(zero_indices, 1)
  
  # Replace the value at that index with 0.1
  data_testing$TotalBsmtSF[random_index] <- 0.1
}

model_custom <- lm(SalePriceLog ~ 
              GrLivArea + LotArea + MasVnrArea + SecondFlrSF + TotalBsmtSF + TotRmsAbvGrd + YearBuilt + YearRemodAdd +
              HasFireplace +  BsmtCond + PoolQC + HasGarage + MasVnrType + Neighborhood + OverallCond + OverallQual + Street + 
              GrLivArea:Neighborhood + HasVnr:MasVnrArea + HasBsmt:TotalBsmtSF + HasSecondFloor:SecondFlrSF
              , data = data_testing)
          
generate_diagnosticPlots(model_custom)

generate_model_stats(model_custom, data_testing)

```


#### Experimental stuff


```{r}

library(quantreg)

# M <- lm(
#   SalePriceLog ~
#     BsmtExposure_Num +
#     BsmtFinSF1 +
#     BsmtFinSF2 +
#     BsmtFinSF2_LOG +
#     BsmtFinType1_Num +
#     BsmtQual_Num +
#     CentralAir +
#     Condition1 +
#     EnclosedPorch +
#     ExterCond_Num +
#     Exterior1st +
#     Fireplaces +
#     Foundation +
#     Functional_Num +
#     GarageArea_LOG +
#     HalfBath +
#     Heating +
#     HeatingQC_Num +
#     KitchenAbvGr +
#     KitchenQual_Num +
#     LandSlope +
#     LotArea_LOG +
#     LotConfig +
#     MasVnrType +
#     MSZoning +
#     Neighborhood +
#     OpenPorchSF +
#     OverallCond +
#     OverallQual +
#     PoolQC_Num +
#     SaleCondition +
#     SaleType +
#     ScreenPorch +
#     Street +
#     ThirdSsnPorch +
#     TotalAreaSF_LOG +
#     TotalAreaSF_SQ +
#     TotalBaths +
#     Utilities +
#     WoodDeckSF +
#     YearBuilt +
#     YearRemodAdd,
#   data = data_all
# )

# Fit a quantile regression at the 15th percentile (tau = 0.15)
mod_qr <- lm(SalePriceLog ~
    BsmtExposure_Num +
    BsmtFinSF1 +
    BsmtFinSF2 +
    BsmtFinSF2_LOG +
    BsmtFinType1_Num +
    BsmtQual_Num +
    CentralAir +
    Condition1 +
    EnclosedPorch +
    ExterCond_Num +
    Exterior1st +
    Fireplaces +
    Foundation +
    Functional_Num +
    GarageArea_LOG +
    HalfBath +
    Heating +
    HeatingQC_Num +
    KitchenAbvGr +
    KitchenQual_Num +
    LandSlope +
    LotArea_LOG +
    LotConfig +
    MasVnrType +
    MSZoning +
    Neighborhood +
    OpenPorchSF +
    OverallCond +
    OverallQual +
    PoolQC_Num +
    SaleCondition +
    SaleType +
    ScreenPorch +
    Street +
    ThirdSsnPorch +
    TotalAreaSF_LOG +
    TotalAreaSF_SQ +
    TotalBaths +
    Utilities +
    WoodDeckSF +
    YearBuilt +
    YearRemodAdd,
  data = data_all, tau = 0.15)

alias(mod_qr)

summary(mod_qr)



cutoff <- quantile(data_all$SalePriceLog, 0.15)
low_price_data <- subset(data_all, SalePriceLog <= cutoff)


weights <- FSelector::information.gain(Q ~ ., data = data_all %>% select(-SalePrice, -SalePriceLog))
# Sort by importance
weights_sorted <- weights[order(weights$attr_importance, decreasing = TRUE), ]

# Take the top 10 variables
top10 <- rownames(weights_sorted)[1:10]
print(top10)

library(FSelectorRcpp)



weights <- linearCorrelation(SalePriceLog ~ ., data = low_price_data %>% select(-SalePrice))
print(weights)

# Then select top features
subset <- cutoff.k(weights, 10)


data_all <- preprocess_data_with_response(data)

bottom_cutoff <- quantile(data_all$SalePriceLog, 0.05, na.rm = TRUE)
data_all$Q5 <- ifelse(data_all$SalePriceLog <= bottom_cutoff, "bottom", "else")
              
bottom_cutoff <- quantile(data_all$SalePriceLog, 0.10, na.rm = TRUE)
data_all$Q10 <- ifelse(data_all$SalePriceLog <= bottom_cutoff, "bottom", "else")

bottom_cutoff <- quantile(data_all$SalePriceLog, 0.15, na.rm = TRUE)
data_all$Q15 <- ifelse(data_all$SalePriceLog <= bottom_cutoff, "bottom", "else")

bottom_cutoff <- quantile(data_all$SalePriceLog, 0.20, na.rm = TRUE)
data_all$Q20 <- ifelse(data_all$SalePriceLog <= bottom_cutoff, "bottom", "else")

explore(data_all %>% select(-SalePrice, -SalePriceLog, -Id, -Q5, -Q10, -Q20))

```


```{r}
library(olsrr)

M <- readRDS('models/forward_fit.rds')

ols_plot_added_variable(M$model)

```

### Fixing "Missing" but really missing data, and fix types

```{r}

library(bestNormalize)

data_clean <- data %>% 
  rename(
    FirstFlrSF = '1stFlrSF',
    SecondFlrSF = '2ndFlrSF',
    ThirdSsnPorch = '3SsnPorch'
  )

#these look like factors
data_clean <- data_clean %>% 
  mutate(
    OverallQual = as.factor(OverallQual),
    OverallCond = as.factor(OverallCond),
    MSSubClass = as.factor(MSSubClass),
    SalePrice = SalePrice,
    SalePriceLog = log(SalePrice),
    PoolQC = ifelse(is.na(PoolQC), 'None', PoolQC),
    MiscFeature = ifelse(is.na(MiscFeature), 'None', MiscFeature),
    Alley = ifelse(is.na(Alley), 'None', Alley),
    Fence = ifelse(is.na(Fence), 'None', Fence),
    FireplaceQu = ifelse(is.na(FireplaceQu), 'None', FireplaceQu),
    GarageYrBlt = ifelse(is.na(GarageYrBlt) & is.na(GarageType), 0, GarageYrBlt),
    GarageType = ifelse(is.na(GarageType), 'None', GarageType),
    GarageFinish = ifelse(is.na(GarageFinish), 'None', GarageFinish),
    GarageQual = ifelse(is.na(GarageQual), 'None', GarageQual),
    GarageCond = ifelse(is.na(GarageCond), 'None', GarageCond),
    BsmtQual = ifelse(is.na(BsmtQual), 'None', BsmtQual),
    BsmtCond = ifelse(is.na(BsmtCond), 'None', BsmtCond),
    BsmtExposure = ifelse(is.na(BsmtExposure), 'None', BsmtExposure),
    BsmtFinType2 = ifelse(is.na(BsmtFinType2), 'None', BsmtFinType2),
    BsmtFinType1 = ifelse(is.na(BsmtFinType1), 'None', BsmtFinType1),
    MasVnrArea = ifelse(MasVnrType == 'None' & is.na(MasVnrArea), 0, MasVnrArea)
  )

data_clean <- data_clean %>% 
  mutate(
    HasGarage = ifelse(GarageType == 'None', 'No', 'Yes'),
    HasBsmt = ifelse(BsmtQual == 'None', 'No', 'Yes'),
    HasFence = ifelse(Fence == 'None', 'No', 'Yes'),
    HasPool = ifelse(PoolQC == 'None', 'No', 'Yes'),
    HasMiscFeature = ifelse(MiscFeature == 'None', 'No', 'Yes'),
    HasAlley = ifelse(Alley == 'None', 'No', 'Yes'),
    HasFireplace = ifelse(FireplaceQu == 'None', 'No', 'Yes'),
    HasSecondFloor = ifelse(SecondFlrSF == 0, 'No', 'Yes'),
    HasVnr = ifelse(MasVnrType == 'None', 'No', 'Yes')
    
  )

#chars to factors
data_clean <- data_clean %>%
  mutate_if(is.character, as.factor)


data_all <- data_clean %>%  dplyr::select(order(colnames(.))) # Order columns by name ascending
```


### Transformations

```{r}

library(bestNormalize)
library(dplyr)


data_all <- data_all %>%
  mutate(

  across(
    matches("(?i)(area|sf|sf1|sf2)$"), # Case-insensitive regex for "area" or "sf"
    ~ log1p(.),
    .names = "{.col}_LOG" # Append "Log" to the column name
  ),
  across(
    matches("(?i)(area|sf|sf1|sf2)$"), # Case-insensitive regex for "area" or "sf"
    ~ ifelse(. == 0, NA, .),
    .names = "{.col}_Only" # Append "Log" to the column name
  )
  )


data_all <- data_all %>%  dplyr::select(order(colnames(.))) # Order columns by name ascending
```


### Imputing Missing Values

It is given that missing values can be treat as missing at random. We will use the missForest package to impute missing values. This package uses a random forest algorithm to impute missing values.

```{r}

library(missForest)
library(doParallel)
library(naniar)




missings <- data_clean %>% 
  dplyr::select(where(~ any(is.na(.))))

gg_miss_var(missings)
gg_miss_var(missings, show_pct = TRUE)

#these columns are approximately missing 50 to 100% of data.  Let's not even try with these. 
#also let's remove the ID column
#excluded_columns <- c('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'Id')

#data_clean <- data_clean %>% select(-all_of(excluded_columns))

set.seed(100)

num_cores <- detectCores() - 2

#impute missing values
cl <- makeCluster(num_cores)
registerDoParallel(cl)

impute_vals <- missForest(data_clean, parallelize = 'variables')

stopCluster(cl)

data_all <- impute_vals$ximp

data_all <- data_clean %>%  dplyr::select(order(colnames(.))) # Order columns by name ascending
```

#### NOT WORKING CODE

Couldn't get this to run right. Maybe come back to it.

```{r}

library(leaps)
library(doParallel)
library(dplyr)

response <- data_all$SalePrice
predictors <- data_all %>% select(-all_of('SalePrice'))

num_cores <- parallel::detectCores() - 2
cl2 <- makeCluster(num_cores, outfile="log.txt")
registerDoParallel(cl2)

run_regsubsets <- function(group, r) {
  regsubsets(x = predictor_group, y = r, nvmax = 1, method = "exhaustive")
}

indices <- rep(1:ceiling(length(names(predictors)) / num_cores), each = num_cores)[1:length(names(predictors))]
groups <- split(names(predictors), indices)

results <- foreach(group = groups, .packages = c("leaps",'dplyr'), .export = c('data_all', 'response', "run_regsubsets")) %do% {
  predictor_group <- data_all %>% select(all_of(group))
  summary(run_regsubsets(group, response))
}

mallows_cp_list <- lapply(results, function(res) res$cp)
mallows_cp <- do.call(c, mallows_cp_list)

# Identify the best predictor (minimum Cp)
best_predictor_idx <- which.min(mallows_cp)
best_predictor <- names(predictors)[best_predictor_idx]
best_cp <- min(mallows_cp)

# Output the best predictor and its Cp value
best_model <- data.frame(
  Predictor = best_predictor,
  Cp = best_cp
)

print(best_model)

# Stop parallel backend
stopCluster(cl2)


```
