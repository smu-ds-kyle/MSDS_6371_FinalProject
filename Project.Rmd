---
title: "Project"
author: "Kyle Evans and Eric Graham"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Helper Functions

```{r}

generate_diagnosticPlots <- function(model){
  
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(model)
par(mfrow = c(1, 1))  # 

plot(model$fitted.values, model$residuals,
     xlab = "Fitted Values",
     ylab = "Residuals",
     main = "Residuals vs Fitted")
abline(h = 0, col = "red")

qqnorm(model$residuals,
       main = "Normal Q-Q Plot")
qqline(model$residuals, col = "red")

influencePlot(model)

hist(model$residuals,
     breaks = 20,
     xlab = "Residuals",
     main = "Histogram of Residuals",
     col = "lightblue")
}


generate_model_stats <- function(model){
  
terms <- all.vars(terms(model))

data_M <- data_all %>% dplyr::select(all_of(terms))

num_cores <- parallel::detectCores() - 2
cl <- makeCluster(num_cores)
registerDoParallel(cl)

ctrl <- trainControl(method = "LOOCV", allowParallel = TRUE, savePredictions = "all")
model <- train(SalePrice ~ ., data = data_M, method = "lm", trControl = ctrl)

cv_predictions <- model$pred
residuals <- cv_predictions$obs - cv_predictions$pred



print(summary(model))
print(model)

aic <- AIC(model$finalModel)
print(str_glue("AIC: {aic} \n"))

bic_value <- BIC(model$finalModel)
print(str_glue("BIC: {bic_value} \n"))

pressStat <- sum(residuals^2)
print(str_glue("PRESS: {pressStat} \n"))

stopCluster(cl)
}

```

## General EDA

Load the data

```{r}

library(data.table)
library(dplyr)

data = fread('data/train.csv')

```

Create a visual GUI to browse the data at will.

```{r}

library(explore)

explore(data)


```

Create an automatic EDA report

```{r}

library(DataExplorer)

DataExplorer::create_report(data, output_file = "Data_Explorer_EDA.html")

```

The EDA report shows some missing data, so let's dive into that deeper.

```{r}

library(naniar)

missings <- data %>% 
  select(where(~ any(is.na(.))))

gg_miss_var(missings)
gg_miss_var(missings, show_pct = TRUE)

```

## Imputing Missing Values

It is given that missing values can be treat as missing at random. We will use the missForest package to impute missing values. This package uses a random forest algorithm to impute missing values.

```{r}

library(missForest)
library(doParallel)

data_clean <- data %>% 
  rename(
    FirstFlrSF = '1stFlrSF',
    SecondFlrSF = '2ndFlrSF',
    ThirdSsnPorch = '3SsnPorch'
  )

#chars to factors
data_clean <- data_clean %>%
  mutate_if(is.character, as.factor)

#these look like factors
data_clean <- data_clean %>% 
  mutate(
    OverallQual = as.factor(OverallQual),
    OverallCond = as.factor(OverallCond),
    MSSubClass = as.factor(MSSubClass),
    SalePrice = log(SalePrice)
  )

#these columns are approximately missing 50 to 100% of data.  Let's not even try with these. 
#also let's remove the ID column
excluded_columns <- c('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'Id')

data_clean <- data_clean %>% select(-all_of(excluded_columns))

set.seed(100)

num_cores <- detectCores() - 2

#impute missing values
cl <- makeCluster(num_cores)
registerDoParallel(cl)

impute_vals <- missForest(data_clean, parallelize = 'variables')

stopCluster(cl)

data_all <- impute_vals$ximp

```


## ANALYSIS 1:

**Assume that Century 21 Ames (a real estate company) in Ames Iowa has commissioned you to answer a very important question with respect to their business. Century 21 Ames only sells houses in the NAmes, Edwards and BrkSide neighborhoods and would like to simply get an estimate of how the SalePrice of the house is related to the square footage of the living area of the house (GrLIvArea) and if the SalesPrice (and its relationship to square footage) depends on which neighborhood the house is located in. Build and fit a model that will answer this question, keeping in mind that realtors prefer to talk about living area in increments of 100 sq. ft. Provide your client with the estimate (or estimates if it varies by neighborhood) as well as confidence intervals for any estimate(s) you provide. It turns out that Century 21â€™s leadership team has a member that has some statistical background. Therefore, make sure and provide evidence that the model assumptions are met and that any suspicious observations (outliers / influential observations) have been identified and addressed. Finally, of course, provide your client with a well written conclusion that quantifies the relationship between living area and sale price with respect to these three neighborhoods. Remember that the company is only concerned with the three neighborhoods they sell in.**

Filter and select the relevant data, then look at assumptions. From initial observatsions on the raw data, there does seem to be a general positive, linear relationship between GrLiveArea and SalePrice. However, the SalePrice data look to be right skewed, particularly in the Edwards and NAmes neighborhoods. Additionally, with the Edwards neighborhood, there seem to be several highly influential and high leverage points. Will readdress these points with a log of SalesPrice in the next code block.

```{r}
library(ggplot2)
library(GGally)

unique(data$Neighborhood)

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide')) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood) %>% 
  dplyr::mutate(
    Neighborhood = as.factor(Neighborhood)
  )

ggpairs(data_filtered)


```

The log transformation of SalePrice has helped a significant amount to settle down outliers and improve normality, however, there do seem to still be two significant outliers in the Edwards neighborhood. NOTE: address this later?

```{r}


data_filtered <- data_filtered %>% 
  mutate(
    SalePrice = log(SalePrice)
  )

ggpairs(data_filtered)


```

For now, keeping outliers in.

As expected, m0 has smallest adjusted R2, not taking into account Neighborhood at all. M1 improves upon M0. Interestingly M2 and M3 have the same adjusted R2, implying that the increased complexity of M3 does not help the model at all and can be left out. M4 and M5 both improve over M2 and M3 and also have the same adjusted R2. Similarly, this suggests that the added complexity of M5 over M4 is unnecessary.

```{r}

library(stringr)

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)




str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

```

Nested Model Significance testing shows exactly what we thought it would above.

```{r}

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

```

Let's take a closer look at the outiers in the Edwards Neighborhood. Properties with ID'S 1299 and 524 are much larger than all the others in the neighborhood, by at least 2x. Unlikely that this is a data recording error, but possible. By removing these two data points, increase the adjusted R2 of our best model (M4) by 8%. In keeping with "all models are wrong but some are useful", it may be presenting this "wrong" model to the company with the caveat that it should only be used with houses 3000 sq ft or below. Very likely the improvement in accuracy is worth this trade off as there does large houses seem to be very rare. This sample suggests 2 out of \~380.

```{r}

data_edwards <- data %>% filter(Neighborhood == "Edwards") %>%  select(order(colnames(.)))
#View(data_edwards)

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide') & !(Id %in% c(1299,524))) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood, Id) %>% 
  dplyr::mutate(
    Neighborhood = as.factor(Neighborhood),
    SalePrice = log(SalePrice)
  )

data_filtered %>% ggplot(aes(x=GrLivArea, y=SalePrice, colour = Neighborhood)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(vars(Neighborhood))

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)


str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

generate_diagnosticPlots(m4)
```

For completeness sake, we can log transform the GrLivArea. This transformation results in the same best model of M4, with practically the same adjusted R2 of .52. In keeping with the idea of parsimony, the easy of interpretation with the log-linear model without outliers, may be more practical than this log-log model with those outliers included.

```{r}

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide')) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood, Id) %>% 
  dplyr::mutate(
    Neighborhood = as.factor(Neighborhood),
    GrLivArea = log(GrLivArea),
    SalePrice = log(SalePrice)
  )

data_filtered %>% ggplot(aes(x=GrLivArea, y=SalePrice, colour = Neighborhood)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(vars(Neighborhood))

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)


str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

generate_diagnosticPlots(m4)
```

Often common to jump to log of money data, but let's check the linear model with no transformations. We can see from the diagnostic plots, that we there is evidence of moderate increasing variance as well as a right skewed histogram of the residuals. These problems are improved upon when the log of the sales price is used.

```{r}

data_filtered <- data %>% 
  dplyr::filter(Neighborhood %in% c('NAmes','Edwards','BrkSide') & !(Id %in% c(1299,524))) %>% 
  dplyr::select(SalePrice, GrLivArea, Neighborhood, Id)

data_filtered %>% ggplot(aes(x=GrLivArea, y=SalePrice, colour = Neighborhood)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  facet_grid(vars(Neighborhood))

#baseline, without neighborhood
m0 = lm(SalePrice ~ GrLivArea, data = data_filtered)
s0 = summary(m0)

#addative effect model, no interaction
m1 = lm(SalePrice ~ GrLivArea + Neighborhood, data = data_filtered)
s1 = summary(m1)

#interaction only model
m2 = lm(SalePrice ~ GrLivArea:Neighborhood, data = data_filtered)
s2 = summary(m2)

#GrLiveArea effect + Interaction
m3 = lm(SalePrice ~ GrLivArea + GrLivArea:Neighborhood, data = data_filtered)
s3 = summary(m3)

#Neighborhood effect + Interaction
m4 = lm(SalePrice ~ Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s4 = summary(m4)

#GrLiveArea and Neighborhood effect + Interaction
m5 = lm(SalePrice ~ GrLivArea + Neighborhood + GrLivArea:Neighborhood, data = data_filtered)
s5 = summary(m5)


str_glue("M0 - Adjusted R2: {s0$adj.r.squared}")
str_glue("M1 - Adjusted R2: {s1$adj.r.squared}")
str_glue("M2 - Adjusted R2: {s2$adj.r.squared}")
str_glue("M3 - Adjusted R2: {s3$adj.r.squared}")
str_glue("M4 - Adjusted R2: {s4$adj.r.squared}")
str_glue("M5 - Adjusted R2: {s5$adj.r.squared}")

t0_1 = anova(m0, m1)  
t1_2 = anova(m1, m2)  
t2_3 = anova(m2, m3) 
t3_4 = anova(m3, m4)  
t4_5 = anova(m4, m5)  

str_glue("M0 vs M1 - Pvalue: {t0_1$`Pr(>F)`[[2]]}")
str_glue("M1 vs M2 - Pvalue: {t1_2$`Pr(>F)`[[2]]}")
str_glue("M2 vs M3 - Pvalue: {t2_3$`Pr(>F)`[[2]]}")
str_glue("M3 vs M4 - Pvalue: {t3_4$`Pr(>F)`[[2]]}")
str_glue("M4 vs M5 - Pvalue: {t4_5$`Pr(>F)`[[2]]}")

generate_diagnosticPlots(m4)

```

## ANALYSIS 2

**Build the most predictive model for sales prices of homes in all of Ames, Iowa.Â  This includes all neighborhoods. Your group is limited to only the techniques we have learned in 6371 (no random forests or other methods we have not yet covered).Â  Specifically, you should produce at least 2 competing models: a simple linear regression model (you pick the explanatory variable) and a multiple linear regression model (SalePrice\~GrLivArea + FullBath) and at least one additional multiple linear regression model where you select the explanatory variables.Â  Generate an adjusted R^2^, CV Press and Kaggle Score for each of these models and clearly describe which model you feel is the best in terms of being able to predict future sale prices of homes in Ames, Iowa.Â  In your paper, please include a table similar to the one below.Â  The group with the lowest public Kaggle score will receive an extra 3 bonus points on the final exam!**Â 


#### Simple Linear Regression Model

To find the best simple linear model, we loop through all the continuous variables and fit a model. To perhaps no surprise, the best model was GrLivArea. Since we already fit this model in analysis one, we moved to the next highest adj r2, which was GarageCars. This model had debatable diagnostic plots, so we moved to the next highest which was GarageArea. This model fits the diagnostic plots very well, except for 4 outliers where the garage size is abnormally large. Like GrlLivArea, we will suggest the use of this model in a range. In this instance, between 0 and 1250 sq ft.

```{r}
library(stringr)
library(car)
library(ggplot2)

data_continuous <- data_all %>% select_if(is.numeric)

response <- data_continuous$SalePrice
predictors <- data_continuous %>% select(-all_of(c('SalePrice','GrLivArea', 'GarageCars')))


formulas1 <- foreach(predictor = names(predictors)) %do%{
  f <- str_glue("SalePrice~{predictor}")
  as.formula(f)
}

formulas2 <- foreach(predictor = names(predictors)) %do%{
  f <- str_glue("SalePrice~log({predictor} + .00001)")
  as.formula(f)
}

formulas <- c(formulas1, formulas2)

results <- foreach(formula = formulas) %do% {
  m = lm(formula, data = data_all)
  summary(m)
}

adj_r2 <- sapply(results, function(res) res$adj.r.squared)

best_index <- which.max(adj_r2)

f <- formulas[[best_index]]

print(f)

results[[best_index]]

```

Diagnostic Plots for GarageArea

```{r}

data_garage <- data_all %>% select(SalePrice, GarageArea)

ggplot(data_garage, aes(x=GarageArea, y=SalePrice)) + geom_point() + geom_smooth(method = 'lm')

m = lm(f, data = data_all)

generate_diagnosticPlots(m)

```

##### M1

Model Statistics and Predictions

```{r}

library(caret)
library(doParallel)

M <- lm(SalePrice ~ GarageArea, data = data_all)

generate_diagnosticPlots(M)

generate_model_stats(M)

```

#### Multiple Linear Regression - Feature Analysis/Selection

##### Continuous Variable Analysis

Looking for multi-collinearity with CAR package. It would seem GrLivArea has perfect collineratity with several other variables, which we will remove.

For GrLivArea: FirstFlrSF, SecondFlrSF, LowQualFinSF

For TotalBsmtSF: BsmtFinSF1, BsmtFinSF2, BsmtUnfSF

```{r}

library(car)

data_continuous <- data_all %>% select_if(is.numeric)
base_model <- lm(SalePrice ~., data = data_continuous)
alias(SalePrice ~., data = data_continuous)

col_to_remove <- c('FirstFlrSF','SecondFlrSF','LowQualFinSF','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF')

data_continuous_clean <- data_continuous %>% dplyr::select(-all_of(col_to_remove))

base_model_clean <- lm(SalePrice ~., data = data_continuous_clean)
vifs <- vif(base_model_clean)

print(vifs)

# # Compute correlation matrix
# cor_matrix <- cor(data_continuous_clean, use = "pairwise.complete.obs")
# 
# # Extract correlations with GrLivArea
# grlivarea_cor <- cor_matrix["GrLivArea", ]
# 
# # Sort correlations in decreasing order
# grlivarea_cor_sorted <- sort(abs(grlivarea_cor), decreasing = TRUE)
# 
# print(grlivarea_cor_sorted)
```


##### Categorical Variable Analysis

Not sure this was helpful as it seems to suggest everything is statistically relevant. Though maybe it would be helpful to take the most relevant ones.

Neighborhood, ExterQual, KitchenQual, Foundation, GarageFinish, BsmtQual, OverallQual, GarageType

```{r}

data_cata <- data_all %>% select_if(is.factor)

print('ANOVA TESTS')
for (var in names(data_cata)) {
  formula <- as.formula(paste("SalePrice ~", var))
  anova_result <- aov(formula, data = data_all)
  p_value <- summary(anova_result)[[1]][["Pr(>F)"]][1]
  if (p_value < 0.05) {
    cat("Variable:", var, "- p-value:", p_value, "\n")
  }
}


print('KRUSKAL-WALLIS TEST')
for (var in names(data_cata)) {
  formula <- as.formula(paste("SalePrice ~", var))
  kruskal_result <- kruskal.test(formula, data = data_all)
  p_value <- kruskal_result$p.value
  cat("Variable:", var, "- p-value:", p_value, "\n")
}


```

##### Combined/Advanced Feature Selection Techniques

MASS library can target AIC by doing stepwise feature selection. Using "Both" in this instance. This will be one model that we test in the paper.

```{r}

library(MASS)

model <- lm(SalePrice ~ ., data = data_all)
set.seed(100)
step_model <- stepAIC(model, direction = "both", trace = FALSE)

```

Limited Leaps as a full run would take too long. This particular run, the same model had the highest CP, BIC, and ADJR2, which was:

YearBuilt, YearRemoAdd, HeatingGasW, HalfBath, GarageQualPo

```{r}

library(leaps)

fit <- regsubsets(SalePrice ~ ., data = data_all, nvmax = 4, nbest = 5, method = 'exhaustive', really.big = TRUE)

fit_sum <- summary(fit)

# Identify the best models according to different metrics
best_adjr2_index <- which.max(fit_sum$adjr2)
best_cp_index <- which.min(fit_sum$cp)
best_bic_index <- which.min(fit_sum$bic)

# Extract coefficients
best_adjr2_model <- coef(fit, id = best_adjr2_index)
best_cp_model <- coef(fit, id = best_cp_index)
best_bic_model <- coef(fit, id = best_bic_index)

print(best_adjr2_model)
print(best_cp_model)
print(best_bic_model)


#par(mfrow = c(1, 2))
#plot(fit, scale = "adjr2", main = "Adjusted R-squared")
```


After removing perfect collinearity, vif() was able to run. Some research seems to suggest that a VIF of greater than 5 show's high correlation to other variables in the mode. I suspect that garagecars and garagearea are related, so let's try removing one of those and checking vif again.

Let's take the continuous columns we found in the VIF analysis and then the most significant categorical variables we found. Can't use leaps on catagorical vars, so trying olsrr.

```{r}

library(olsrr)
library(dplyr)
library(car)

data_continuous <- data_all %>% select_if(is.numeric)

col_to_remove <- c('FirstFlrSF','SecondFlrSF','LowQualFinSF','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'GarageArea')
data_continuous_clean_t1 <- data_continuous %>% dplyr::select(-all_of(col_to_remove))
base_model_clean_t1<- lm(SalePrice ~., data = data_continuous_clean_t1)
vifs_t1 <- vif(base_model_clean_t1) 
print(vifs_t1)

sig_cata <- c('Neighborhood', 'ExterQual', 'KitchenQual', 'Foundation', 'GarageFinish', 'BsmtQual', 'OverallQual', 'GarageType')

continuous_cols <- data_all %>% select(all_of(names(vifs_t1)))
cata_cols <- data_all %>% select(all_of(sig_cata))

candidate_data <- cbind(continuous_cols, cata_cols, SalePrice = data_all$SalePrice)

#model_mat <- model.matrix(~ ., data = candidate_data)[, -1]

model <- lm(SalePrice ~ ., data = candidate_data)

set.seed(100)
fit_forward <- ols_step_forward_adj_r2(model)

set.seed(100)
fit_backward <- ols_step_backward_adj_r2(model)

set.seed(100)
fit_both <- ols_step_both_adj_r2(model)

set.seed(100)
fit_heir <- ols_step_both_p(model, .05, heirarchical = TRUE)

plot(fit_forward)
plot(fit_backward)
plot(fit_both)
plot(fit_heir)

saveRDS(fit_forward, 'models/forward_fit.rds')
saveRDS(fit_both, 'models/fit_both.rds')
saveRDS(fit_heir, 'models/fit_heir.rds')

```

Research suggested using Boruta as a way to suggest features. This did not seem to help narrow it down at all, but to be fair, we haven't studied this and I'm not entirely sure what it is doing, but was hopeful it might point in the right direction. The idea was that if x number of selection techniques generally agreed, that would be a good sign.

```{r}

library(Boruta)

set.seed(100)

boruta_out <- Boruta(SalePrice ~ ., data = data_all, doTrace = 0)

final_vars_tenative <- getSelectedAttributes(boruta_out, withTentative = TRUE)
final_vars <- getSelectedAttributes(boruta_out, withTentative = FALSE)

print('finals')
print(final_vars)

print('finals-with tentative')
print(final_vars)

```

LASSO keeps coming up in my research for data sets with a large number of features. This look cool, not again, not sure what exactly it's doing as we haven't studied it.

```{r}

library(glmnet)

x <- model.matrix(SalePrice ~ ., data = data_all)[, -1]
y <- data_all$SalePrice

lasso_cv <- cv.glmnet(x, y, alpha = 1)

optimal_lambda <- lasso_cv$lambda.min

lasso_model <- glmnet(x, y, alpha = 1, lambda = optimal_lambda)

lasso_coefficients <- coef(lasso_model)

print(lasso_coefficients)

```

#### Multiple Linear Regression - Models

##### M1

Found by MASS package, targeting AIC. Seems somewhat problematic perhaps. Histogram of residuals looks great and the residual scatter plot is decent, but the QQ plot looks mildly concerning as well as the leverage plot. Oddly,

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/potential_model_1.rds')

generate_diagnosticPlots(M)

generate_model_stats(M)


```


##### M2

Found by olsrr package, targeting adjr2 with forward selection on subjectively selected data

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/forward_fit.rds')

generate_diagnosticPlots(M$model)

generate_model_stats(M$model)

```

##### M3

Found by olsrr package, targeting adjr2 with both selection on subjectively selected data

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/fit_both.rds')

generate_diagnosticPlots(M$model)

generate_model_stats(M$model)

```

##### M4

Found by olsrr package, targeting adjr2 with both heir selection on subjectively selected data

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/fit_heir.rds')

generate_diagnosticPlots(M$model)

generate_model_stats(M$model)

```

##### M5

Found by olsrr package, targeting adjr2 with forward selection on all the data

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/forward_fit_all.rds')

generate_diagnosticPlots(M$model)

generate_model_stats(M$model)

```

##### M6

Found by olsrr package, targeting adjr2 with both selection on all the data

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/fit_both_all.rds')

generate_diagnosticPlots(M$model)

generate_model_stats(M$model)

```

##### M7

Found by olsrr package, targeting adjr2 with both heir selection on all the data

```{r}

library(caret)
library(doParallel)

M <- readRDS('models/fit_heir_all.rds')

generate_diagnosticPlots(M$model)

generate_model_stats(M$model)

```

#### NOT WORKING CODE

Couldn't get this to run right. Maybe come back to it.

```{r}

library(leaps)
library(doParallel)
library(dplyr)

response <- data_all$SalePrice
predictors <- data_all %>% select(-all_of('SalePrice'))

num_cores <- parallel::detectCores() - 2
cl2 <- makeCluster(num_cores, outfile="log.txt")
registerDoParallel(cl2)

run_regsubsets <- function(group, r) {
  regsubsets(x = predictor_group, y = r, nvmax = 1, method = "exhaustive")
}

indices <- rep(1:ceiling(length(names(predictors)) / num_cores), each = num_cores)[1:length(names(predictors))]
groups <- split(names(predictors), indices)

results <- foreach(group = groups, .packages = c("leaps",'dplyr'), .export = c('data_all', 'response', "run_regsubsets")) %do% {
  predictor_group <- data_all %>% select(all_of(group))
  summary(run_regsubsets(group, response))
}

mallows_cp_list <- lapply(results, function(res) res$cp)
mallows_cp <- do.call(c, mallows_cp_list)

# Identify the best predictor (minimum Cp)
best_predictor_idx <- which.min(mallows_cp)
best_predictor <- names(predictors)[best_predictor_idx]
best_cp <- min(mallows_cp)

# Output the best predictor and its Cp value
best_model <- data.frame(
  Predictor = best_predictor,
  Cp = best_cp
)

print(best_model)

# Stop parallel backend
stopCluster(cl2)


```
